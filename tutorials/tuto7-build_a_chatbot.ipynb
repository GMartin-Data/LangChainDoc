{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [**this LangChain tutorial**](https://python.langchain.com/v0.2/docs/tutorials/chatbot/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
    "- [**Conversational RAG**](https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/): Enable a chatbot experience over an external source of data.\n",
    "- [**Agent**](https://python.langchain.com/v0.2/docs/tutorials/agents/): Build a chatbot that can take actions.\n",
    "\n",
    "This tutorial will cover the basics which will be helpful for those two more advanced topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few of the high-level components we will be working with:\n",
    "- [**Chat Models**](https://python.langchain.com/v0.2/docs/concepts/#chat-models). The chatbot interface is based around messages rather than raw text, and therefore is best suited to Chat Models rather than text LLMs.\n",
    "- [**Prompt Templates**](https://python.langchain.com/v0.2/docs/concepts/#prompt-templates), which simplify the process of assembling prompts that combine:\n",
    "    - default messages,\n",
    "    - user input,\n",
    "    - chat history,\n",
    "    - (optionally) additional retrieved content\n",
    "- [**Chat History**](https://python.langchain.com/v0.2/docs/concepts/#chat-history), which allows a chatbot to \"remember\" past interactions and take them into account, when responding to follow-up questions\n",
    "- Debugging and tracking your application using [**LangSmith**](https://python.langchain.com/v0.2/docs/concepts/#langsmith)\n",
    "\n",
    "We'll cover how to fit the above components together to create a powerful conversational chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow pretty printing\n",
    "from rich import print as rprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOpenAI</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">client</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">openai.resources.chat.completions.Completions</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x758b731f7210</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">async_client</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;openai.resources.chat.completions.AsyncCompletions object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x758b72c1a6d0</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">openai_api_key</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SecretStr</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'**********'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">openai_proxy</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatOpenAI\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mclient\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mopenai.resources.chat.completions.Completions\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x758b731f7210\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33masync_client\u001b[0m\u001b[39m=<openai.resources.chat.completions.AsyncCompletions object at \u001b[0m\u001b[1;36m0x758b72c1a6d0\u001b[0m\u001b[1m>\u001b[0m,\n",
       "    \u001b[33mopenai_api_key\u001b[0m=\u001b[1;35mSecretStr\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'**********'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mopenai_proxy\u001b[0m=\u001b[32m''\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first use the model directly.\n",
    "\n",
    "`ChatModel`s are instances of LangChain *Runnables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(langchain_openai.chat_models.base.ChatOpenAI,\n",
       " langchain_openai.chat_models.base.BaseChatOpenAI,\n",
       " langchain_core.language_models.chat_models.BaseChatModel,\n",
       " langchain_core.language_models.base.BaseLanguageModel,\n",
       " langchain_core.runnables.base.RunnableSerializable,\n",
       " langchain_core.load.serializable.Serializable,\n",
       " pydantic.v1.main.BaseModel,\n",
       " pydantic.v1.utils.Representation,\n",
       " langchain_core.runnables.base.Runnable,\n",
       " typing.Generic,\n",
       " abc.ABC,\n",
       " object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__class__.__mro__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This means **they expose a [standard interface](https://python.langchain.com/v0.1/docs/expression_language/interface/) for interacting with them**.\n",
    "\n",
    "To just simply call the model, we can pass in a list of messages to the `.invoke` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Hello Bob! How can I assist you today?'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-b15ff913-87b0-4a9c-97de-844527bc3afd-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'Hello Bob! How can I assist you today?'\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m10\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m13\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m23\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "        \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run-b15ff913-87b0-4a9c-97de-844527bc3afd-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m13\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m10\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m23\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(model.invoke([HumanMessage(content=\"Hi! I'm Bob.\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **API Reference: [HumanMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model, on its own, **does not have any concept of state**.\n",
    "\n",
    "For example, if you ask a followup question like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"I'm sorry, I don't have access to that information.\"</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-a4016a93-3722-41cf-8cf4-43de19a2368d-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m\"I\u001b[0m\u001b[32m'm sorry, I don't have access to that information.\"\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m13\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m12\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m25\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "        \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run-a4016a93-3722-41cf-8cf4-43de19a2368d-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m12\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m13\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m25\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(model.invoke([HumanMessage(content=\"What's my name?\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this example [**LangSmith trace**](https://smith.langchain.com/public/5c21cb92-2814-4119-bae9-d02b8db577ac/r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that **it doesn't take the previous conversation turn into context, and then cannot answer the last question**.\n",
    "\n",
    "This makes for a terrible chatbot experience!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get aroung this, we need to **pass the entire conversation history into the model**.\n",
    "\n",
    "Let's see what happens in case we do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Your name is Bob.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-8d31d262-6ccc-468e-8d6b-cd13b9717c4d-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'Your name is Bob.'\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m5\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m35\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m40\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "        \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run-8d31d262-6ccc-468e-8d6b-cd13b9717c4d-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m35\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m5\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m40\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(\n",
    "    model.invoke([\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can see that we get a good response!\n",
    "\n",
    "This is the basic idea underpinning a chatbot's ability to interact conversationnaly.\n",
    "\n",
    "So, **how do we best implement this**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **use a Message History class to wrap our model and make it stateful**.\n",
    "\n",
    "This will:\n",
    "- **keep track of inputs and outputs of the model**,\n",
    "- **store them in some datastore**.\n",
    "\n",
    "Future interactions will then:\n",
    "- **load those messages**,\n",
    "- **pass them into the chain as part of the input**.\n",
    "\n",
    "Let's see how to use this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can:\n",
    "- import the relevant classes,\n",
    "- set up our chain which wraps the model and adds in this message history.\n",
    "\n",
    "**A key part here is the function we pass into as the `get_session_history`**.\n",
    "\n",
    "This function **is expected to**:\n",
    "- take in a `session_id`\n",
    "- return a Message History object.\n",
    "\n",
    "This `session_id`:\n",
    "- is used to **distinguish between separate conversations**,\n",
    "- should be **passed in as part of the config when calling the new chain**. (We'll show how to do that) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory  # Will be stored\n",
    "from langchain_core.chat_history import BaseChatMessageHistory  # Only used for type hint\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory  # Will be used to instanciate the wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict which will store the session_ids as keys\n",
    "# and corresponding chat messages as values\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"Chat history retriever, given a session_id\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.history.RunnableWithMessageHistory"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(with_message_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(langchain_core.runnables.history.RunnableWithMessageHistory,\n",
       " langchain_core.runnables.base.RunnableBindingBase,\n",
       " langchain_core.runnables.base.RunnableSerializable,\n",
       " langchain_core.load.serializable.Serializable,\n",
       " pydantic.v1.main.BaseModel,\n",
       " pydantic.v1.utils.Representation,\n",
       " langchain_core.runnables.base.Runnable,\n",
       " typing.Generic,\n",
       " abc.ABC,\n",
       " object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.__class__.__mro__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **API Reference**\n",
    "- [**ChatMessageHistory**](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.ChatMessageHistory.html)\n",
    "- [**BaseChatMessageHistory**](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html)\n",
    "- [**RunnableWithMessageHistory**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **IMPORTANT**\n",
    "> \n",
    "> **We now need to create a `config` that we pass into the runnable every time**.\n",
    "\n",
    "This config contains information that is not part of the input directly, but is still useful.\n",
    "\n",
    "In this case, we want to include a `session_id`. This should look like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"ObiwanKenobi\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, Bob! How can I assist you today?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Bob.\")],\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Bob.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our chatbot is now able to \"remember\" things about us.\n",
    "\n",
    "If we change the config to reference a different `session_id`, we can start a conversation from fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have the capability to know your name.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"Luke\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can still go back to the original conversation, as we're persisting it in `store`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Bob.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"ObiwanKenobi\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we can support a chatbot having conversations with many users!\n",
    "\n",
    "Right now, all we-ve done is add a simple persistence layer around the model.\n",
    "\n",
    "We can start to make the more complicated and personalized by adding in a **prompt template**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt templates** help to **turn raw user information into a format that the LLM can work with**.\n",
    "\n",
    "In this case, the raw user input is just a message, that we are passing to the LLM.\n",
    "\n",
    "Let's now make it a bit more complicated.\n",
    "- First, let's add in a **system message** with some custom instructions (but still taking messages as input).\n",
    "- Next, we'll add in more input besides just as messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do this, we will:\n",
    "- Create a `ChatPromptTemplate`, and through its `from_messages` method pass a list of messages:\n",
    "- first, a **system message** we will define,\n",
    "- then, we will make use of the `MessagePlaceholder` class to pass all the messages in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompts.chat.ChatPromptTemplate"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**\n",
    ">\n",
    "> The prompt objects have a `pretty_print` method that helps display their internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant. Answer all questions to the best of your ability.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **API Reference**\n",
    "- [**ChatPromptTemplate**](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)\n",
    "- [**MessagesPlaceholder**](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's chain all prompt and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**\n",
    "> \n",
    "> This slightly changes the input type.\n",
    "\n",
    "Rather than pass in a list of messages, we now have to pass a `dict` with a `messages` key, whose value is a `list` of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Hello, Bob! How can I assist you today?'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">44</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-4b6e466d-c2fa-4896-a58b-97f5689b1d5e-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">44</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'Hello, Bob! How can I assist you today?'\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m11\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m33\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m44\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "        \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run-4b6e466d-c2fa-4896-a58b-97f5689b1d5e-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m33\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m11\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m44\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(\n",
    "    chain.invoke({\n",
    "        \"messages\": [HumanMessage(content=\"Hi! I'm Bob.\")]\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order to make chat history persist, we can wrap this the same way as before**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"DarkVador\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, Jim! How can I assist you today?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Jim.\")],\n",
    "    config = config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**\n",
    "> \n",
    "> Notice that **we've come back to our first way of passing messages within a list**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Jim.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ObiwanKenobi'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">InMemoryChatMessageHistory</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Hi! I'm Bob.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Hello, Bob! How can I assist you today?'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-274de4a9-6015-4c11-abb9-392dee6f8e81-0'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"What's my name?\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Your name is Bob.'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-8c666426-7d96-4012-9020-e7935c7684d7-0'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span><span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"What's my name?\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Your name is Bob.'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-b5d0d418-b022-4109-bf65-0ca2e6b57582-0'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Luke'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">InMemoryChatMessageHistory</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"What's my name?\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"I'm sorry, I do not have the capability to know your name.\"</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-7b38c8c6-60e2-417f-ade6-8f1cd40f11c1-0'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span><span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'DarkVador'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">InMemoryChatMessageHistory</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Hi! I'm Jim.\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Hello, Jim! How can I assist you today?'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">44</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-0001bb2e-ea07-4024-af05-4757402add30-0'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">44</span><span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"What's my name?\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Your name is Jim.'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">62</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-161e0ef5-0b6a-4d6e-88c3-d5d867f5a0a0-0'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">62</span><span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'ObiwanKenobi'\u001b[0m: \u001b[1;35mInMemoryChatMessageHistory\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m\"Hi\u001b[0m\u001b[32m! I'm Bob.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m'Hello, Bob! How can I assist you today?'\u001b[0m,\n",
       "                \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m11\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m13\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m24\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "                    \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                    \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mid\u001b[0m=\u001b[32m'run-274de4a9-6015-4c11-abb9-392dee6f8e81-0'\u001b[0m,\n",
       "                \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m13\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m11\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m24\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m\"What\u001b[0m\u001b[32m's my name?\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m'Your name is Bob.'\u001b[0m,\n",
       "                \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m5\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m37\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m42\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "                    \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                    \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mid\u001b[0m=\u001b[32m'run-8c666426-7d96-4012-9020-e7935c7684d7-0'\u001b[0m,\n",
       "                \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m37\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m5\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m42\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m\"What\u001b[0m\u001b[32m's my name?\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m'Your name is Bob.'\u001b[0m,\n",
       "                \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m5\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m55\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m60\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "                    \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                    \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mid\u001b[0m=\u001b[32m'run-b5d0d418-b022-4109-bf65-0ca2e6b57582-0'\u001b[0m,\n",
       "                \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m55\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m5\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m60\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'Luke'\u001b[0m: \u001b[1;35mInMemoryChatMessageHistory\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m\"What\u001b[0m\u001b[32m's my name?\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m\"I\u001b[0m\u001b[32m'm sorry, I do not have the capability to know your name.\"\u001b[0m,\n",
       "                \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m15\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m12\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m27\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "                    \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                    \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mid\u001b[0m=\u001b[32m'run-7b38c8c6-60e2-417f-ade6-8f1cd40f11c1-0'\u001b[0m,\n",
       "                \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m12\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m15\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m27\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'DarkVador'\u001b[0m: \u001b[1;35mInMemoryChatMessageHistory\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m\"Hi\u001b[0m\u001b[32m! I'm Jim.\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m'Hello, Jim! How can I assist you today?'\u001b[0m,\n",
       "                \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m11\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m33\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m44\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "                    \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                    \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mid\u001b[0m=\u001b[32m'run-0001bb2e-ea07-4024-af05-4757402add30-0'\u001b[0m,\n",
       "                \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m33\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m11\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m44\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m\"What\u001b[0m\u001b[32m's my name?\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m'Your name is Jim.'\u001b[0m,\n",
       "                \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m5\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m57\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m62\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "                    \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                    \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mid\u001b[0m=\u001b[32m'run-161e0ef5-0b6a-4d6e-88c3-d5d867f5a0a0-0'\u001b[0m,\n",
       "                \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m57\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m5\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m62\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Just to check the conversations store\n",
    "rprint(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Let's now make our prompt a little bit more complicated.\n",
    "\n",
    "Let's assume that the prompt template now looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability in {language}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'messages'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'messages'</span>: typing.List<span style=\"font-weight: bold\">[</span>typing.Union<span style=\"font-weight: bold\">[</span>langchain_core.messages.ai.AIMessage, \n",
       "langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, \n",
       "langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, \n",
       "langchain_core.messages.tool.ToolMessage<span style=\"font-weight: bold\">]]</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'language'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'You are a helpful assistant. Answer all questions to the best of your ability in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{language}'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MessagesPlaceholder</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">variable_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'messages'</span><span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOpenAI</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">client</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">openai.resources.chat.completions.Completions</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x758b731f7210</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #808000; text-decoration-color: #808000\">async_client</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;openai.resources.chat.completions.AsyncCompletions object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x758b72c1a6d0</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">openai_api_key</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SecretStr</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'**********'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">openai_proxy</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'language'\u001b[0m, \u001b[32m'messages'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'messages'\u001b[0m: typing.List\u001b[1m[\u001b[0mtyping.Union\u001b[1m[\u001b[0mlangchain_core.messages.ai.AIMessage, \n",
       "langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, \n",
       "langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, \n",
       "langchain_core.messages.tool.ToolMessage\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mSystemMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'language'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[33mtemplate\u001b[0m=\u001b[32m'You are a helpful assistant. Answer all questions to the best of your ability in \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32mlanguage\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[1;35mMessagesPlaceholder\u001b[0m\u001b[1m(\u001b[0m\u001b[33mvariable_name\u001b[0m=\u001b[32m'messages'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mChatOpenAI\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mclient\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mopenai.resources.chat.completions.Completions\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x758b731f7210\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33masync_client\u001b[0m\u001b[39m=<openai.resources.chat.completions.AsyncCompletions object at \u001b[0m\u001b[1;36m0x758b72c1a6d0\u001b[0m\u001b[1m>\u001b[0m,\n",
       "        \u001b[33mopenai_api_key\u001b[0m=\u001b[1;35mSecretStr\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'**********'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mopenai_proxy\u001b[0m=\u001b[32m''\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **we have include a new `language` input to the prompt (specified as `input_variables` in the previous display)**\n",
    "\n",
    "We can now invoke the chain **and pass in a language of our choice**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola, Bob! En qu puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Hi! I'm Bob.\")],\n",
    "    \"language\": \"Spanish\"\n",
    "})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now wrap it in a Message History class.\n",
    "\n",
    "> **IMPORTANT**\n",
    "> \n",
    "> This time, **because there are multiple keys in the input, we need to specify the correct key to use to save the chat history**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"Jarjar Bings\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola Todd! En qu puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": \"Hi! I'm Todd!\",\n",
    "        \"language\": \"Spanish\",\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tu nombre es Todd.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": \"What's my name?\",\n",
    "        \"language\": \"Spanish\",\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Conversation History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important concept when building chatbots is **managing conversation history**.\n",
    "\n",
    "If left unmanaged, **the list of messages will grow unbounded and potentially overflow the context window of the LLM**\n",
    "\n",
    "Therefore, it is important to **add a step that limits the size of messages you're passing in**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **IMPORTANT**\n",
    "> \n",
    "> You will want to do this\n",
    "> - BEFORE the prompt template,\n",
    "> - AFTER you load previous messages from Message History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're here study **one simple way** to do this, others of course exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this by:\n",
    "- **adding a single step in front of the prompt that modifies the `\"messages\"` key appropriately**,\n",
    "- wrap this new chain in the Message History class.\n",
    "\n",
    "First, let's **define a function that will modify the messages passed in**.\n",
    "\n",
    "Let's make it so that **it selects the `k` most recent messages**.\n",
    "\n",
    "We can then create a new chain by adding that step at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_messages(messages, k=10):\n",
    "    \"\"\"Filter the last k messages from a list of messages.\"\"\"\n",
    "    return messages[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=lambda dico: filter_messages(dico[\"messages\"]))\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableAssign</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">mapper</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableParallel</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">steps__</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'messages'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableLambda</span><span style=\"font-weight: bold\">(</span>lambda dico: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">filter_messages</span><span style=\"font-weight: bold\">(</span>dico<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'messages'</span><span style=\"font-weight: bold\">]))}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'language'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'messages'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'messages'</span>: typing.List<span style=\"font-weight: bold\">[</span>typing.Union<span style=\"font-weight: bold\">[</span>langchain_core.messages.ai.AIMessage, \n",
       "langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, \n",
       "langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, \n",
       "langchain_core.messages.tool.ToolMessage<span style=\"font-weight: bold\">]]</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'language'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'You are a helpful assistant. Answer all questions to the best of your ability in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{language}'</span>\n",
       "                    <span style=\"font-weight: bold\">)</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MessagesPlaceholder</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">variable_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'messages'</span><span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOpenAI</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">client</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">openai.resources.chat.completions.Completions</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x758b731f7210</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #808000; text-decoration-color: #808000\">async_client</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;openai.resources.chat.completions.AsyncCompletions object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x758b72c1a6d0</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">openai_api_key</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SecretStr</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'**********'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">openai_proxy</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mRunnableAssign\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmapper\u001b[0m=\u001b[1;35mRunnableParallel\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33msteps__\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'messages'\u001b[0m: \u001b[1;35mRunnableLambda\u001b[0m\u001b[1m(\u001b[0mlambda dico: \u001b[1;35mfilter_messages\u001b[0m\u001b[1m(\u001b[0mdico\u001b[1m[\u001b[0m\u001b[32m'messages'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'language'\u001b[0m, \u001b[32m'messages'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'messages'\u001b[0m: typing.List\u001b[1m[\u001b[0mtyping.Union\u001b[1m[\u001b[0mlangchain_core.messages.ai.AIMessage, \n",
       "langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, \n",
       "langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, \n",
       "langchain_core.messages.tool.ToolMessage\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mSystemMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'language'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                        \u001b[33mtemplate\u001b[0m=\u001b[32m'You are a helpful assistant. Answer all questions to the best of your ability in \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32mlanguage\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[1;35mMessagesPlaceholder\u001b[0m\u001b[1m(\u001b[0m\u001b[33mvariable_name\u001b[0m=\u001b[32m'messages'\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mChatOpenAI\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mclient\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mopenai.resources.chat.completions.Completions\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x758b731f7210\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33masync_client\u001b[0m\u001b[39m=<openai.resources.chat.completions.AsyncCompletions object at \u001b[0m\u001b[1;36m0x758b72c1a6d0\u001b[0m\u001b[1m>\u001b[0m,\n",
       "        \u001b[33mopenai_api_key\u001b[0m=\u001b[1;35mSecretStr\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'**********'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mopenai_proxy\u001b[0m=\u001b[32m''\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DOCUMENTATION**: [**assign: add values to state**](https://python.langchain.com/v0.1/docs/expression_language/primitives/assign/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **API Reference**: [**RunnablePassthrough**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try it out!\n",
    "\n",
    "If we create a list of messages with more than 10 messages long, we can see that it no longer remembers information from the earliest messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't have access to that information.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we ask information about the remaining messages, it still remembers it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your favorite ice cream flavor is vanilla.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what's my fav ice cream\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now wrap this in the Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"Dokku\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't have access to your personal information.\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now two new messages in the chat history.\n",
    "\n",
    "That means that even more information that used to be accessible in our conversation history is no longer available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't have that information.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"whats my favorite ice cream?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a look at LangSmith, you can see exactly what is happening under the hood in the [**LangSmith Trace**](https://smith.langchain.com/public/fa6b00da-bcd8-4c1c-a799-6b32a3d62964/r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've got a functional chatbot.\n",
    "\n",
    "However, one *really* important UX consideration for chatbot application is **streaming**.\n",
    "\n",
    "Indeed, LLMs can sometimes take a while to fully respond. So, in order to improve the user experience, one thing that most applications do is **stream back each token as soon as it is generated**, which **allows user to concretely see progress**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that it's actually super easy to do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All chains expose a `.stream` method**, and ones that use message history are no different.\n",
    "\n",
    "We can simply use that method to get back a streaming response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Bien| sr|,| Todd|!| Vo|ici| une| bl|ague| pour| vous| :| Pour|quoi| les| pl|ong|eurs| pl|ong|ent|-|ils| toujours| en| ar|rire| et| jamais| en| avant| ?| Par|ce| que| sinon| ils| tomb|ent| toujours| dans| le| b|ateau| !| ||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"Leia\"}}\n",
    "\n",
    "for r in with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Hi! I'm Todd. Could you please tell me a joke?\")],\n",
    "        \"language\": \"French\",\n",
    "    },\n",
    "    config=config\n",
    "):\n",
    "    print(r.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
