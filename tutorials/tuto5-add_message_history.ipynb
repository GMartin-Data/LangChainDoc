{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Message History (Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RunnableWithMessageHistory` lets us **add message history to certain types of chains**.\n",
    "\n",
    "It:\n",
    "- wraps another Runnable,\n",
    "- manages the chat message history for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, it can be used for any runnable that:\n",
    "-  **takes as input one of**:\n",
    "    - a sequence of `BaseMessage`,\n",
    "    - a `dict` with a key that takes a sequence of `BaseMessage`,\n",
    "    - a `dict` with a key that takes:\n",
    "        - the latest message(s) as a string or sequence of `BaseMessage`, and\n",
    "        - a separate key that takes historical messages.\n",
    "- **and returns as output one of**:\n",
    "    - a string that can be treated as the contents of an `AIMessage`,\n",
    "    - a sequence of `BaseMessage`,\n",
    "    - a `dict` with a key that contains a sequence of `BaseMessage`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some examples to see how it works.\n",
    "\n",
    "First, we construct a runnable which, here:\n",
    "- accepts a `dict` as input,\n",
    "- returns a message as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You're an assistant who's good at {ability}. Respond in 20 words or fewer.\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **API Reference**:\n",
    "> - [**ChatPromptTemplate**](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)\n",
    "> - [**MessagesPlaceholder**](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n",
    "> - [**ChatOpenAI**](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manage the message history, we will need:\n",
    "1. this runnable,\n",
    "2. a callable that returns an instance of `BaseChatMessageHistory`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out the [**memory integrations**](https://integrations.langchain.com/memory) page for implementations of chat message histories using Redis and other providers.\n",
    "\n",
    "Here, we will demonstrate using an **in-memory** `ChatMessageHistory` as well as more persistent storage using `RedisChatMessageHistory`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we show a simple example in which the chat history lives in memory, in this case, via a global python `dict`.\n",
    "\n",
    "We construct a callable `get_session_history` that references this `dict` to return an instance of `ChatMessageHistory`.\n",
    "\n",
    "The arguments to the callable can be specified by passing a **configuration** to the `RunnableWithMessageHistory` at runtime.\n",
    "\n",
    "By default, the configuration parameter is expected to be a single string `session_id`.\n",
    "\n",
    "This can be adjusted via the `history_factory_config` kwarg.\n",
    "\n",
    "Using the signel-parameter default..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key = \"input\",\n",
    "    history_messages_key = \"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **API Reference**: WARNING, THESE LINKS HAVE TO BE FIXED\n",
    "> - [**ChatMessageHistory**](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.ChatMessageHistory.html)\n",
    "> - [**BaseChatMessageHistory**](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html)\n",
    "> - [**RunnableWithMessageHistory**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have specified:\n",
    "- `input_message_key` (the key to be treated as the latest input message),\n",
    "- `history_message_key` (the key to add historical messages to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When invoking this new runnable, we specify the corresponding chat history via a **configuration** parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A trigonometric function that gives the ratio of the adjacent side to the hypotenuse in a right triangle.', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 34, 'total_tokens': 57}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-384752b0-4a9b-4542-bf38-6df6ffd007ac-0', usage_metadata={'input_tokens': 34, 'output_tokens': 23, 'total_tokens': 57})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"ability\": \"math\",\n",
    "        \"input\": \"What does cosine mean?\"\n",
    "    },\n",
    "    # The following is mandatory, except, of course, the session_id's value\n",
    "    config = {\"configurable\": {\"session_id\": \"abc123\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **And now it *remembers*...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine is a function in trigonometry that relates the angle of a right triangle to its side lengths.', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 67, 'total_tokens': 89}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-65223bcc-d937-42b9-bbc2-37b948350800-0', usage_metadata={'input_tokens': 67, 'output_tokens': 22, 'total_tokens': 89})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"ability\": \"math\",\n",
    "        \"input\": \"What?\"\n",
    "    },\n",
    "    config = {\"configurable\": {\"session_id\": \"abc123\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Obviously, with a new `session_id`, it doesn't remember...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I can help with math problems. Just ask me anything math-related, and I'll do my best to assist you.\", response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 31, 'total_tokens': 55}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d289ea11-70e3-4367-a8fb-ec68da065fde-0', usage_metadata={'input_tokens': 31, 'output_tokens': 24, 'total_tokens': 55})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"ability\": \"math\",\n",
    "        \"input\": \"What?\"\n",
    "    },\n",
    "    config = {\"configurable\": {\"session_id\": \"def234\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration parameters by which we track messaeg histories **can be customized by passing in a `list` of `ConfigurableFieldSpec` objects to the `history_factory_config` parameter.\n",
    "\n",
    "Below, we use two parameters:\n",
    "- `user_id`,\n",
    "- `conversation_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        store[(user_id, conversation_id)] = ChatMessageHistory()\n",
    "    return store[(user_id, conversation_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key = \"input\",\n",
    "    history_messages_key = \"history\",\n",
    "    # Here's the config\n",
    "    history_factory_config = [\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"user_id\",\n",
    "            annotation = str,\n",
    "            name = \"User ID\",\n",
    "            description = \"Unique identifier for the user.\",\n",
    "            default = \"\",\n",
    "            is_shared = True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"conversation_id\",\n",
    "            annotation = str,\n",
    "            name = \"Conversation ID\",\n",
    "            description = \"Unique identifier for the conversation.\",\n",
    "            default = \"\",\n",
    "            is_shared = True,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **API Reference**:\n",
    "> - [**ConfigurableFieldSpec**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.utils.ConfigurableFieldSpec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you with math today?', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 30, 'total_tokens': 41}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c40797cb-bb71-4fd8-a7c0-153c78d62e05-0', usage_metadata={'input_tokens': 30, 'output_tokens': 11, 'total_tokens': 41})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"ability\": \"math\",\n",
    "        \"input\": \"Hello\"\n",
    "    },\n",
    "    config = {\"configurable\": {\n",
    "        \"user_id\": \"123\",\n",
    "        \"conversation_id\": \"1\"\n",
    "    }},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
