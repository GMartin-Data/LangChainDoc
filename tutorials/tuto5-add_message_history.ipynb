{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Message History (Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RunnableWithMessageHistory` lets us **add message history to certain types of chains**.\n",
    "\n",
    "It:\n",
    "- wraps another Runnable,\n",
    "- manages the chat message history for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, it can be used for any runnable that:\n",
    "-  **takes as input one of**:\n",
    "    - a sequence of `BaseMessage`,\n",
    "    - a `dict` with a key that takes a sequence of `BaseMessage`,\n",
    "    - a `dict` with a key that takes:\n",
    "        - the latest message(s) as a string or sequence of `BaseMessage`, and\n",
    "        - a separate key that takes historical messages.\n",
    "- **and returns as output one of**:\n",
    "    - a string that can be treated as the contents of an `AIMessage`,\n",
    "    - a sequence of `BaseMessage`,\n",
    "    - a `dict` with a key that contains a sequence of `BaseMessage`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some examples to see how it works.\n",
    "\n",
    "First, we construct a runnable which, here:\n",
    "- accepts a `dict` as input,\n",
    "- returns a message as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You're an assistant who's good at {ability}. Respond in 20 words or fewer.\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **API Reference**:\n",
    "> - [**ChatPromptTemplate**](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)\n",
    "> - [**MessagesPlaceholder**](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n",
    "> - [**ChatOpenAI**](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manage the message history, we will need:\n",
    "1. this runnable,\n",
    "2. a callable that returns an instance of `BaseChatMessageHistory`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out the [**memory integrations**](https://integrations.langchain.com/memory) page for implementations of chat message histories using Redis and other providers.\n",
    "\n",
    "Here, we will demonstrate using an **in-memory** `ChatMessageHistory` as well as more persistent storage using `RedisChatMessageHistory`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we show a simple example in which the chat history lives in memory, in this case, via a global python `dict`.\n",
    "\n",
    "We construct a callable `get_session_history` that references this `dict` to return an instance of `ChatMessageHistory`.\n",
    "\n",
    "The arguments to the callable can be specified by passing a **configuration** to the `RunnableWithMessageHistory` at runtime.\n",
    "\n",
    "By default, the configuration parameter is expected to be a single string `session_id`.\n",
    "\n",
    "This can be adjusted via the `history_factory_config` kwarg.\n",
    "\n",
    "Using the signel-parameter default..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key = \"input\",\n",
    "    history_messages_key = \"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **API Reference**: WARNING, THESE LINKS HAVE TO BE FIXED\n",
    "> - [**ChatMessageHistory**](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.ChatMessageHistory.html)\n",
    "> - [**BaseChatMessageHistory**](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html)\n",
    "> - [**RunnableWithMessageHistory**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have specified:\n",
    "- `input_message_key` (the key to be treated as the latest input message),\n",
    "- `history_message_key` (the key to add historical messages to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When invoking this new runnable, we specify the corresponding chat history via a **configuration** parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A trigonometric function that gives the ratio of the adjacent side to the hypotenuse in a right triangle.', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 34, 'total_tokens': 57}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-384752b0-4a9b-4542-bf38-6df6ffd007ac-0', usage_metadata={'input_tokens': 34, 'output_tokens': 23, 'total_tokens': 57})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"ability\": \"math\",\n",
    "        \"input\": \"What does cosine mean?\"\n",
    "    },\n",
    "    # The following is mandatory, except, of course, the session_id's value\n",
    "    config = {\"configurable\": {\"session_id\": \"abc123\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **And now it *remembers*...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine is a function in trigonometry that relates the angle of a right triangle to its side lengths.', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 67, 'total_tokens': 89}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-65223bcc-d937-42b9-bbc2-37b948350800-0', usage_metadata={'input_tokens': 67, 'output_tokens': 22, 'total_tokens': 89})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"ability\": \"math\",\n",
    "        \"input\": \"What?\"\n",
    "    },\n",
    "    config = {\"configurable\": {\"session_id\": \"abc123\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Obviously, with a new `session_id`, it doesn't remember...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I can help with math problems. Just ask me anything math-related, and I'll do my best to assist you.\", response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 31, 'total_tokens': 55}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d289ea11-70e3-4367-a8fb-ec68da065fde-0', usage_metadata={'input_tokens': 31, 'output_tokens': 24, 'total_tokens': 55})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"ability\": \"math\",\n",
    "        \"input\": \"What?\"\n",
    "    },\n",
    "    config = {\"configurable\": {\"session_id\": \"def234\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration parameters by which we track messaeg histories **can be customized by passing in a `list` of `ConfigurableFieldSpec` objects to the `history_factory_config` parameter.\n",
    "\n",
    "Below, we use two parameters:\n",
    "- `user_id`,\n",
    "- `conversation_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        store[(user_id, conversation_id)] = ChatMessageHistory()\n",
    "    return store[(user_id, conversation_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key = \"input\",\n",
    "    history_messages_key = \"history\",\n",
    "    # Here's the config\n",
    "    history_factory_config = [\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"user_id\",\n",
    "            annotation = str,\n",
    "            name = \"User ID\",\n",
    "            description = \"Unique identifier for the user.\",\n",
    "            default = \"\",\n",
    "            is_shared = True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id = \"conversation_id\",\n",
    "            annotation = str,\n",
    "            name = \"Conversation ID\",\n",
    "            description = \"Unique identifier for the conversation.\",\n",
    "            default = \"\",\n",
    "            is_shared = True,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **API Reference**:\n",
    "> - [**ConfigurableFieldSpec**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.utils.ConfigurableFieldSpec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you with math today?', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 30, 'total_tokens': 41}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c40797cb-bb71-4fd8-a7c0-153c78d62e05-0', usage_metadata={'input_tokens': 30, 'output_tokens': 11, 'total_tokens': 41})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"ability\": \"math\",\n",
    "        \"input\": \"Hello\"\n",
    "    },\n",
    "    config = {\"configurable\": {\n",
    "        \"user_id\": \"123\",\n",
    "        \"conversation_id\": \"1\"\n",
    "    }},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples With Runnables of Different Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above runnable:\n",
    "- takes a `dict` as input,\n",
    "- returns a `BaseMessage`.\n",
    "\n",
    "Below, we show some alternatives..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messages input, `dict` Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_message': AIMessage(content=\"Simone de Beauvoir believed that individuals have the ability to make free choices and shape their own lives through their actions. She argued that while people are influenced by social, cultural, and historical factors, they still have the power to choose how they respond to these influences and create their own unique paths. Beauvoir emphasized the importance of taking responsibility for one's choices and actions, and believed that true freedom comes from actively engaging with the world and taking control of one's own destiny.\", response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 21, 'total_tokens': 117}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e9889fea-2277-4047-8d2a-e6b8382151d5-0', usage_metadata={'input_tokens': 21, 'output_tokens': 96, 'total_tokens': 117})}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableParallel({\"output_message\": ChatOpenAI()})\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    output_messages_key = \"output_message\",\n",
    ")\n",
    "\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\">hat did Simeone De Beauvoir believe about free will?\")],\n",
    "    config = {\"configurable\": {\"session_id\": \"baz\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **API Reference**:\n",
    "> - [**HumanMessage**](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html)\n",
    "> - [**RunnableParallel**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableParallel.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_message': AIMessage(content=\"Simone de Beauvoir's views on free will are closely aligned with those of Jean-Paul Sartre, her longtime partner and philosophical collaborator. Both Beauvoir and Sartre were existentialist philosophers who believed in the idea of radical freedom and the importance of individual choice in shaping one's existence.\\n\\nLike Beauvoir, Sartre argued that human beings are fundamentally free to choose their own paths and create their own meaning in a seemingly indifferent universe. Both philosophers rejected determinism and emphasized the importance of personal responsibility and agency in the face of external influences.\\n\\nHowever, there are some differences between Beauvoir and Sartre's perspectives on free will. For example, Beauvoir's feminist perspective led her to explore the ways in which social structures, particularly gender roles, can limit women's freedom and agency. In contrast, Sartre's focus was more on the individual's struggle to define themselves in the face of existential angst and the burden of freedom.\\n\\nOverall, while Beauvoir and Sartre shared a belief in the importance of free will and personal responsibility, Beauvoir's feminist perspective added a unique dimension to her ideas about freedom and agency.\", response_metadata={'token_usage': {'completion_tokens': 232, 'prompt_tokens': 133, 'total_tokens': 365}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-aca77aca-cfca-4d55-abc4-0c739e1ae10d-0', usage_metadata={'input_tokens': 133, 'output_tokens': 232, 'total_tokens': 365})}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"How did this compare to Sartre\")],\n",
    "    config={\"configurable\": {\"session_id\": \"baz\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messages Input, Messages Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x74a1a019efb0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x74a1a01c4700>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x74a1a9184160>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x74a1a018a8c0>, history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnableWithMessageHistory(\n",
    "    ChatOpenAI(),\n",
    "    get_session_history,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `dict` with Single Key for All Messages Input, Messages Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  input_messages: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "}), config={'run_name': 'insert_history'})\n",
       "| RunnableBinding(bound=RunnableLambda(itemgetter('input_messages'))\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x74a1a0139930>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x74a1a01c4790>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x74a1a01d0a60>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x74a1a018a8c0>, input_messages_key='input_messages', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnableWithMessageHistory(\n",
    "    itemgetter(\"input_messages\") | ChatOpenAI(),\n",
    "    get_session_history,\n",
    "    input_messages_key = \"input_messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
