{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence and Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [**this tutorial**](https://learn.deeplearning.ai/courses/ai-agents-in-langgraph/lesson/5/persistence-and-streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Basic Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate Search Tool\n",
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AgentState\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deal with **persistence**, we will use what's called a **checkpointer** into LangGraph.\n",
    "\n",
    "It basically **checkpoints the state between every node**.\n",
    "\n",
    "Here, we'll make use of a `SqliteSaver`, and use it \"in memory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's then really easy to incorporate it within our `Agent` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Agent\n",
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):  # 👈 REFERENCE TO CHECKPOINTER\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)  # 👈 REFERENCE TO CHECKPOINTER\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**\n",
    ">\n",
    "> For data persistence, it can also be made use of other databases, or [**Redis**](https://redis.io/) for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")  # 👈 Change to -4o for POC\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now implement **threads**, in order to keep track of different conversations.\n",
    "\n",
    "They can simply be configured in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now call the graph, not with `invoke`, but with `stream`, passing:\n",
    "- the same dictionary,\n",
    "- `thread` as a second argument.\n",
    "\n",
    "We're then gonna get back a **stream** of events.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is here to temporarily manage an issue with LangSmith, don't bother with understanding it for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LANGCHAIN_TRACING_V2=\"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_eqDDo3U0wuFrWTitkRpMCIw4', 'function': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 1186, 'total_tokens': 1207}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-af070dca-58fc-42d1-9d5b-c2d081e72a86-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_eqDDo3U0wuFrWTitkRpMCIw4'}], usage_metadata={'input_tokens': 1186, 'output_tokens': 21, 'total_tokens': 1207})]\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_eqDDo3U0wuFrWTitkRpMCIw4'}\n",
      "Back to the model!\n",
      "[ToolMessage(content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717751204, \\'localtime\\': \\'2024-06-07 2:06\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717750800, \\'last_updated\\': \\'2024-06-07 02:00\\', \\'temp_c\\': 12.2, \\'temp_f\\': 54.0, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Clear\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/113.png\\', \\'code\\': 1000}, \\'wind_mph\\': 4.3, \\'wind_kph\\': 6.8, \\'wind_degree\\': 10, \\'wind_dir\\': \\'N\\', \\'pressure_mb\\': 1011.0, \\'pressure_in\\': 29.84, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 93, \\'cloud\\': 0, \\'feelslike_c\\': 10.7, \\'feelslike_f\\': 51.2, \\'windchill_c\\': 9.8, \\'windchill_f\\': 49.6, \\'heatindex_c\\': 11.4, \\'heatindex_f\\': 52.6, \\'dewpoint_c\\': 9.3, \\'dewpoint_f\\': 48.8, \\'vis_km\\': 14.0, \\'vis_miles\\': 8.0, \\'uv\\': 1.0, \\'gust_mph\\': 14.0, \\'gust_kph\\': 22.5}}\"}, {\\'url\\': \\'https://www.weather.gov/index.php/mtr/\\', \\'content\\': \\'Current Conditions showing NA; Customize Your Weather.gov. Enter Your City, ST or ZIP Code ... 2024 at 9:40:09 am PDT Watches, Warnings & Advisories. Zoom Out. Excessive Heat Warning. Gale Warning. Heat Advisory. Small Craft Advisory. ... National Weather Service San Francisco Bay Area, CA 21 Grace Hopper Ave, Stop 5 Monterey, CA 93943-5505\\'}]', name='tavily_search_results_json', tool_call_id='call_eqDDo3U0wuFrWTitkRpMCIw4')]\n",
      "[AIMessage(content='The current weather in San Francisco is clear with a temperature of 54.0°F (12.2°C). The wind is blowing at 4.3 mph from the north, and the humidity is at 93%.', response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 1732, 'total_tokens': 1778}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-de9261f7-2481-4194-b1ac-0e9e5cebaf7b-0', usage_metadata={'input_tokens': 1732, 'output_tokens': 46, 'total_tokens': 1778})]\n"
     ]
    }
   ],
   "source": [
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back a stream of events:\n",
    "\n",
    "> - **first, we get an `AIMessage`, which is the first result from the language model**.\n",
    "\n",
    "```python\n",
    "AIMessage(\n",
    "    content='',\n",
    "    additional_kwargs={\n",
    "        'tool_calls': [{\n",
    "            'id': 'call_eqDDo3U0wuFrWTitkRpMCIw4',\n",
    "            'function': {\n",
    "                'arguments': '{\"query\":\"weather in San Francisco\"}',\n",
    "                'name': 'tavily_search_results_json'}, \n",
    "                'type': 'function'\n",
    "            }]\n",
    "        }, \n",
    "    response_metadata={\n",
    "        'token_usage': {'completion_tokens': 21, 'prompt_tokens': 1186, 'total_tokens': 1207}, \n",
    "        'model_name': 'gpt-3.5-turbo', \n",
    "        'system_fingerprint': None, \n",
    "        'finish_reason': 'tool_calls', \n",
    "        'logprobs': None\n",
    "        }, \n",
    "    id='run-af070dca-58fc-42d1-9d5b-c2d081e72a86-0', \n",
    "    tool_calls=[{\n",
    "        'name': 'tavily_search_results_json', \n",
    "        'args': {'query': 'weather in San Francisco'}, \n",
    "        'id': 'call_eqDDo3U0wuFrWTitkRpMCIw4'\n",
    "        }], \n",
    "    usage_metadata={'input_tokens': 1186, 'output_tokens': 21, 'total_tokens': 1207}\n",
    ")\n",
    "```\n",
    "> - **It tells us to call `tavily`, which is logged with this printing**:\n",
    "\n",
    "```python\n",
    "Calling: {\n",
    "    'name': 'tavily_search_results_json',\n",
    "    'args': {'query': 'weather in San Francisco'},\n",
    "    'id': 'call_eqDDo3U0wuFrWTitkRpMCIw4'\n",
    "}\n",
    "```\n",
    "> - **Then, the action is performed, is logged with the printing of `Back to the model!`, and we get the following `ToolMessage` (I won't parse it now as it doesn't really improves readability), which is the result of calling `tavily` and, hence, the result of the search**:\n",
    "\n",
    "```python\n",
    "ToolMessage(\n",
    "    content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717751204, \\'localtime\\': \\'2024-06-07 2:06\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717750800, \\'last_updated\\': \\'2024-06-07 02:00\\', \\'temp_c\\': 12.2, \\'temp_f\\': 54.0, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Clear\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/113.png\\', \\'code\\': 1000}, \\'wind_mph\\': 4.3, \\'wind_kph\\': 6.8, \\'wind_degree\\': 10, \\'wind_dir\\': \\'N\\', \\'pressure_mb\\': 1011.0, \\'pressure_in\\': 29.84, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 93, \\'cloud\\': 0, \\'feelslike_c\\': 10.7, \\'feelslike_f\\': 51.2, \\'windchill_c\\': 9.8, \\'windchill_f\\': 49.6, \\'heatindex_c\\': 11.4, \\'heatindex_f\\': 52.6, \\'dewpoint_c\\': 9.3, \\'dewpoint_f\\': 48.8, \\'vis_km\\': 14.0, \\'vis_miles\\': 8.0, \\'uv\\': 1.0, \\'gust_mph\\': 14.0, \\'gust_kph\\': 22.5}}\"}, {\\'url\\': \\'https://www.weather.gov/index.php/mtr/\\', \\'content\\': \\'Current Conditions showing NA; Customize Your Weather.gov. Enter Your City, ST or ZIP Code ... 2024 at 9:40:09 am PDT Watches, Warnings & Advisories. Zoom Out. Excessive Heat Warning. Gale Warning. Heat Advisory. Small Craft Advisory. ... National Weather Service San Francisco Bay Area, CA 21 Grace Hopper Ave, Stop 5 Monterey, CA 93943-5505\\'}]', \n",
    "    name='tavily_search_results_json', \n",
    "    tool_call_id='call_eqDDo3U0wuFrWTitkRpMCIw4'\n",
    ")\n",
    "```\n",
    "> - **Finally, there's an `AIMessage`, which is the result of the LLM, answering our question**:\n",
    "\n",
    "```python\n",
    "AIMessage(\n",
    "    content='The current weather in San Francisco is clear with a temperature of 54.0°F (12.2°C). The wind is blowing at 4.3 mph from the north, and the humidity is at 93%.', \n",
    "    response_metadata={\n",
    "        'token_usage': {'completion_tokens': 46, 'prompt_tokens': 1732, 'total_tokens': 1778}, \n",
    "        'model_name': 'gpt-3.5-turbo', \n",
    "        'system_fingerprint': None, \n",
    "        'finish_reason': 'stop', \n",
    "        'logprobs': None\n",
    "    }, \n",
    "    id='run-de9261f7-2481-4194-b1ac-0e9e5cebaf7b-0', \n",
    "    usage_metadata={'input_tokens': 1732, 'output_tokens': 46, 'total_tokens': 1778}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this `stream` method, we get back all of these intermediate results, and we have a good visibility of what exactly is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow-Up Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_qqHKrIxFv4OIpxO5p1JiekdC', 'function': {'arguments': '{\"query\":\"weather in Los Angeles\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 1790, 'total_tokens': 1811}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-bf5f9952-2066-46b4-9051-8985d832bc51-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in Los Angeles'}, 'id': 'call_qqHKrIxFv4OIpxO5p1JiekdC'}], usage_metadata={'input_tokens': 1790, 'output_tokens': 21, 'total_tokens': 1811})]}\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'weather in Los Angeles'}, 'id': 'call_qqHKrIxFv4OIpxO5p1JiekdC'}\n",
      "Back to the model!\n",
      "{'messages': [ToolMessage(content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'Los Angeles\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 34.05, \\'lon\\': -118.24, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717752570, \\'localtime\\': \\'2024-06-07 2:29\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717751700, \\'last_updated\\': \\'2024-06-07 02:15\\', \\'temp_c\\': 15.6, \\'temp_f\\': 60.1, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 9.4, \\'wind_kph\\': 15.1, \\'wind_degree\\': 130, \\'wind_dir\\': \\'SE\\', \\'pressure_mb\\': 1012.0, \\'pressure_in\\': 29.88, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 93, \\'cloud\\': 100, \\'feelslike_c\\': 15.6, \\'feelslike_f\\': 60.1, \\'windchill_c\\': 18.5, \\'windchill_f\\': 65.3, \\'heatindex_c\\': 18.5, \\'heatindex_f\\': 65.3, \\'dewpoint_c\\': 14.3, \\'dewpoint_f\\': 57.8, \\'vis_km\\': 9.7, \\'vis_miles\\': 6.0, \\'uv\\': 1.0, \\'gust_mph\\': 13.9, \\'gust_kph\\': 22.3}}\"}, {\\'url\\': \\'https://www.wunderground.com/hourly/us/ca/los-angeles/90004/date/2024-6-7\\', \\'content\\': \\'Los Angeles Weather Forecasts. Weather Underground provides local & long-range weather forecasts, weatherreports, maps & tropical weather conditions for the Los Angeles area. ... Hourly Forecast ...\\'}]', name='tavily_search_results_json', tool_call_id='call_qqHKrIxFv4OIpxO5p1JiekdC')]}\n",
      "{'messages': [AIMessage(content='The current weather in Los Angeles is misty with a temperature of 60.1°F (15.6°C). The wind is blowing at 9.4 mph from the southeast, and the humidity is at 93%.', response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 2305, 'total_tokens': 2352}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-eca43777-6cbc-4b60-a7a6-9bff391c6000-0', usage_metadata={'input_tokens': 2305, 'output_tokens': 47, 'total_tokens': 2352})]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What about in la?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **continuing the conversation from before, with asking a follow-up question**.\n",
    "\n",
    "This isn't explicitely mentioned within the query, but it will work **as we mentioned the same `thread_id`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Previous Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='Los Angeles is slightly warmer than San Francisco. Los Angeles currently has a temperature of 60.1°F (15.6°C), while San Francisco has a temperature of 54.0°F (12.2°C).', response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 2364, 'total_tokens': 2410}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-576f4331-b701-4b2c-882c-deb54d982348-0', usage_metadata={'input_tokens': 2364, 'output_tokens': 46, 'total_tokens': 2410})]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing `thread_id` ➡️ **REWORK IT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**\n",
    "> \n",
    "> Strangely, it works... 😱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_6pT9LoHPIkJHlQxXZBPSgCLT', 'function': {'arguments': '{\"query\": \"temperature in Los Angeles\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_UgrAKwuct414HYa99uk6cxN8', 'function': {'arguments': '{\"query\": \"temperature in New York City\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 1171, 'total_tokens': 1229}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7b73501c-14fb-4cdf-ab30-95e199de4fcc-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'temperature in Los Angeles'}, 'id': 'call_6pT9LoHPIkJHlQxXZBPSgCLT'}, {'name': 'tavily_search_results_json', 'args': {'query': 'temperature in New York City'}, 'id': 'call_UgrAKwuct414HYa99uk6cxN8'}], usage_metadata={'input_tokens': 1171, 'output_tokens': 58, 'total_tokens': 1229})]}\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'temperature in Los Angeles'}, 'id': 'call_6pT9LoHPIkJHlQxXZBPSgCLT'}\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'temperature in New York City'}, 'id': 'call_UgrAKwuct414HYa99uk6cxN8'}\n",
      "Back to the model!\n",
      "{'messages': [ToolMessage(content=\"[{'url': 'https://www.accuweather.com/en/us/los-angeles/90012/weather-forecast/347625', 'content': 'Los Angeles, CA Weather Forecast, with current conditions, wind, air quality, and what to expect for the next 3 days.'}, {'url': 'https://forecast.weather.gov/zipcity.php?inputstring=Los Angeles,CA', 'content': 'Downtown Los Angeles Weather Station has Moved; Read our Forecast Discussion Here; Current conditions at NA Lat: NA°NLon: NA°WElev: NAft. N/A. Humidity: NA: Wind Speed: NA: Barometer: NA: ... Los Angeles CA 34.05°N 118.25°W (Elev. 377 ft) Last Update: 2:17 pm PDT Jun 5, 2024. Forecast Valid: 7pm PDT Jun 5, 2024-6pm PDT Jun 12, 2024 .'}]\", name='tavily_search_results_json', tool_call_id='call_6pT9LoHPIkJHlQxXZBPSgCLT'), ToolMessage(content='[{\\'url\\': \\'https://www.accuweather.com/en/us/new-york/10021/weather-forecast/14-349727_1_al\\', \\'content\\': \\'Get the current and future weather conditions for New York City, including temperature, precipitation, air quality, and allergy outlook. See the hourly and 10-day forecast for sun, cloud, and storms.\\'}, {\\'url\\': \\'https://www.accuweather.com/en/us/new-york/10021/weather-forecast/349727\\', \\'content\\': \"TODAY’S WEATHER FORECAST\\\\nLooking Ahead\\\\nExpect rainy weather late tonight through tomorrow evening\\\\nTONIGHT’S WEATHER FORECAST\\\\nTOMORROW’S WEATHER FORECAST\\\\nNew York Weather Radar\\\\nFurther Ahead\\\\nHourly\\\\nDaily\\\\nMonthly\\\\nAround the Globe\\\\nHurricane Tracker\\\\nSevere Weather\\\\nRadar & Maps\\\\nNews\\\\nVideo\\\\nWinter Center\\\\nTop Stories\\\\nWinter Weather\\\\nWeekend storm to dish wintry weather to parts of Midwest, Northeast\\\\n3 hours ago\\\\nAstronomy\\\\nGeminids to outshine all other astronomy events in December\\\\n2 days ago\\\\nWeather News\\\\nOfficials release simulated images of man found dead in Lake Mead\\\\n1 day ago\\\\nAstronomy\\\\nFirst ever planet-forming disk spotted around star in another galaxy\\\\n1 day ago\\\\nWeather Forecasts\\\\nAtmospheric river to unleash month’s worth of rain in Northwest\\\\n3 hours ago\\\\nFeatured Stories\\\\nWeather News\\\\nGhostly new figures appear in the waters of the Caribbean\\\\n1 day ago\\\\nWeather News\\\\nResearchers find \\'witch bottles\\' washed up on Texas beaches\\\\n2 days ago\\\\nWeather News\\\\nGiant seamount found is twice the height of world’s tallest building\\\\n1 day ago\\\\nWeather News\\\\n5 Native American inventions that are still used in our modern world\\\\n3 days ago\\\\nHealth\\\\nWalking could lower your risk of type 2 diabetes\\\\n3 days ago\\\\nWeather Near New York:\\\\nWe have updated our Privacy Policy and Cookie Policy. New York, NY\\\\nNew York\\\\nNew York\\\\nAround the Globe\\\\nHurricane Tracker\\\\nSevere Weather\\\\nRadar & Maps\\\\nNews & Features\\\\nAstronomy\\\\nBusiness\\\\nClimate\\\\nHealth\\\\nRecreation\\\\nSports\\\\nTravel\\\\nVideo\\\\nWinter Center\\\\nToday\\\\nCurrent Weather\\\\n6:30 PM\\\\nCurrent Air Quality\\\\nToday\\\\n12/2\\\\nUnhealthy\\\\nHealth effects can be immediately felt by sensitive groups. Based on Current Pollutants\\\\nMore Details\\\\nLearn more at\\\\nUnhealthy\\\\nHealth effects can be immediately felt by sensitive groups. Based on Current Pollutants\\\\nCurrent Air Quality\\\\nOur current air quality index (AQI) provides information on the quality of air that you are breathing and its impact on your health. There are at least six different pollutants that we track that impact the cleanliness of air and your health.\\\\n\"}]', name='tavily_search_results_json', tool_call_id='call_UgrAKwuct414HYa99uk6cxN8')]}\n",
      "{'messages': [AIMessage(content='The current temperature in Los Angeles, California is not provided in the search results. In New York City, the temperature is not explicitly mentioned, but based on the information available, there is no mention of unusually warm weather. Therefore, it is likely that New York City is not significantly warmer than Los Angeles at the moment.', response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 2141, 'total_tokens': 2206}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9b867bd2-8857-466b-8d64-3a0a45cb2297-0', usage_metadata={'input_tokens': 2141, 'output_tokens': 65, 'total_tokens': 2206})]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "except ImportError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous code doesn't work, it can be fixed with installing [**aiosqlite**](https://pypi.org/project/aiosqlite/), as mentioned in [**AsyncSqliteSaver's documentation**](https://langchain-ai.github.io/langgraph/reference/checkpoints/#asyncsqlitesaver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "The SqliteSaver does not support async methods. Consider using AsyncSqliteSaver instead.\nfrom langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\nNote: AsyncSqliteSaver requires the aiosqlite package to use.\nInstall with:\n`pip install aiosqlite`\nSee https://langchain-ai.github.io/langgraph/reference/checkpoints/#asyncsqlitesaverfor more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the weather in SF?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      2\u001b[0m thread \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m abot\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mastream_events({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages}, thread, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m     kind \u001b[38;5;241m=\u001b[39m event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_chat_model_stream\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/projects/LangChainDoc/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1137\u001b[0m, in \u001b[0;36mRunnable.astream_events\u001b[0;34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnly versions \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of the schema is currently supported.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1135\u001b[0m     )\n\u001b[0;32m-> 1137\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m event_stream:\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "File \u001b[0;32m~/projects/LangChainDoc/.venv/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py:638\u001b[0m, in \u001b[0;36m_astream_events_implementation_v1\u001b[0;34m(runnable, input, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m root_name \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, runnable\u001b[38;5;241m.\u001b[39mget_name())\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# Ignoring mypy complaint about too many different union combinations\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# This arises because many of the argument types are unions\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m _astream_log_implementation(  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     runnable,\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    641\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    642\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    643\u001b[0m     diff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    644\u001b[0m     with_streamed_output_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    646\u001b[0m ):\n\u001b[1;32m    647\u001b[0m     run_log \u001b[38;5;241m=\u001b[39m run_log \u001b[38;5;241m+\u001b[39m log\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m encountered_start_event:\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;66;03m# Yield the start event for the root runnable.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/LangChainDoc/.venv/lib/python3.11/site-packages/langchain_core/tracers/log_stream.py:637\u001b[0m, in \u001b[0;36m_astream_log_implementation\u001b[0;34m(runnable, input, config, stream, diff, with_streamed_output_list, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# Wait for the runnable to finish, if not cancelled (eg. by break)\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m task\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/LangChainDoc/.venv/lib/python3.11/site-packages/langchain_core/tracers/log_stream.py:591\u001b[0m, in \u001b[0;36m_astream_log_implementation.<locals>.consume_astream\u001b[0;34m()\u001b[0m\n\u001b[1;32m    588\u001b[0m prev_final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    589\u001b[0m final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m runnable\u001b[38;5;241m.\u001b[39mastream(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    592\u001b[0m     prev_final_output \u001b[38;5;241m=\u001b[39m final_output\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/LangChainDoc/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1098\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1095\u001b[0m processes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes}\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;66;03m# get checkpoint from saver, or create an empty one\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m saved \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer\u001b[38;5;241m.\u001b[39maget_tuple(config)\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m )\n\u001b[1;32m   1102\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m saved\u001b[38;5;241m.\u001b[39mcheckpoint \u001b[38;5;28;01mif\u001b[39;00m saved \u001b[38;5;28;01melse\u001b[39;00m empty_checkpoint()\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;66;03m# merge configurable fields with previous checkpoint config\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/LangChainDoc/.venv/lib/python3.11/site-packages/langgraph/checkpoint/sqlite.py:401\u001b[0m, in \u001b[0;36mSqliteSaver.aget_tuple\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maget_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[CheckpointTuple]:\n\u001b[1;32m    395\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a checkpoint tuple from the database asynchronously.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    Note:\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m        This async method is not supported by the SqliteSaver class.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m        Use get_tuple() instead, or consider using [AsyncSqliteSaver](#asyncsqlitesaver).\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(_AIO_ERROR_MSG)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The SqliteSaver does not support async methods. Consider using AsyncSqliteSaver instead.\nfrom langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\nNote: AsyncSqliteSaver requires the aiosqlite package to use.\nInstall with:\n`pip install aiosqlite`\nSee https://langchain-ai.github.io/langgraph/reference/checkpoints/#asyncsqlitesaverfor more information."
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            # Empty content in the context of OpenAI means\n",
    "            # that the model is asking for a tool to be invoked.\n",
    "            # So we only print non-empty content\n",
    "            print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
