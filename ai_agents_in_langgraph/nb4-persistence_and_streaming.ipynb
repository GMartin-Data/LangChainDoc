{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence and Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [**this tutorial**](https://learn.deeplearning.ai/courses/ai-agents-in-langgraph/lesson/5/persistence-and-streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building agents, they're often working on **longer running tasks**.\n",
    "\n",
    "For this type of tasks, there are two important concepts:\n",
    "- **persistence**,\n",
    "- **streaming**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Persistence** lets you keep around the state of an agent at a particular point in time. This lets you:\n",
    "> - go back to that state,\n",
    "> - resume in that state in future interactions.\n",
    "\n",
    "> With **streaming**, you can emit a list of signals of what's going on at that exact moment. So, for long running applications, you know exactly what the agent is doing.\n",
    "\n",
    "In this notebook, we will see these concepts in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Basic Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from rich import print as rprint  # Enhance pretty printing for outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate Search Tool\n",
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AgentState\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deal with **persistence**, we will use what's called a **checkpointer** into LangGraph.\n",
    "\n",
    "It basically **checkpoints the state between every node**.\n",
    "\n",
    "Here, we'll make use of a `SqliteSaver`, and use it **\"in memory\"**. This basically means that, when leaving the notebook, memory will be erased. Of course, we could keep track of the previous states with connecting this to an external database (some checkpointers, for example, use *Postgres* or *Redis*). We won't do it as this is a POC and we only want to illustrate the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's then really easy to incorporate it within our `Agent` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Agent\n",
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):  # 👈 REFERENCE TO CHECKPOINTER\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)  # 👈 REFERENCE TO CHECKPOINTER\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**\n",
    ">\n",
    "> For data persistence, it can also be made use of other databases, or [**Redis**](https://redis.io/) for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")  # 👈 Change to -4o for POC\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)  # 👈 Passing the memory object previously defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are mainly two things that we might care about streaming:\n",
    "- First, we might care about **streaming the individual messages**, so that would be:\n",
    "    - the AI message that determines what action to take,\n",
    "    - the observation message that represents the result of taking that action.\n",
    "- The second thing we might take care about streaming is **tokens**. So, for each token of the LLM call, we might want to stream the output.\n",
    "\n",
    "To begin, we're just going to start by streaming the messages. We'll do the tokens later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to add the concept of a **thread config**, which will be used to keep track of different theads, inside the persistent checkpointer.\n",
    "\n",
    "This will allow us to **have multiple conversations going on at the same time**, which is really needed for production applications, where you generally have many users.\n",
    "\n",
    "This **thread config** is only a `dict`, with a `\"configurable\"` key. Its value is another `dict` with a lone key being `\"thread_id\"`, and its value as a `str`, here `\"1\"` (`uuid`s can also be used.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now call the graph, not with `invoke`, but with `stream`, passing:\n",
    "- the same dictionary,\n",
    "- `thread` as a second argument.\n",
    "\n",
    "We're then gonna get back a **stream** of events, which represent **updates to that state over time**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily manage issue with LangSmith (don't bother with this ftm)\n",
    "!export LANGCHAIN_TRACING_V2=\"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'tool_calls'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'call_AOSKlG76P36uUTAwuCHGA3mA'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'arguments'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'{\"query\":\"weather in San Francisco\"}'</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tavily_search_results_json'</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>\n",
       "                <span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2008</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2029</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tool_calls'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-7b5bc405-513c-435c-a754-2b31e2977c83-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tavily_search_results_json'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'weather in San Francisco'</span><span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'call_AOSKlG76P36uUTAwuCHGA3mA'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2008</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2029</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'tool_calls'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'id'\u001b[0m: \u001b[32m'call_AOSKlG76P36uUTAwuCHGA3mA'\u001b[0m,\n",
       "                    \u001b[32m'function'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'arguments'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"query\":\"weather in San Francisco\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                        \u001b[32m'name'\u001b[0m: \u001b[32m'tavily_search_results_json'\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'type'\u001b[0m: \u001b[32m'function'\u001b[0m\n",
       "                \u001b[1m}\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m21\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m2008\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m2029\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "            \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'tool_calls'\u001b[0m,\n",
       "            \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-7b5bc405-513c-435c-a754-2b31e2977c83-0'\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'name'\u001b[0m: \u001b[32m'tavily_search_results_json'\u001b[0m,\n",
       "                \u001b[32m'args'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'query'\u001b[0m: \u001b[32m'weather in San Francisco'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'id'\u001b[0m: \u001b[32m'call_AOSKlG76P36uUTAwuCHGA3mA'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m2008\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m21\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m2029\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_AOSKlG76P36uUTAwuCHGA3mA'}\n",
      "Back to the model!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToolMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'San </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">-122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1718164021, \\'localtime\\': \\'2024-06-11 20:47\\'},</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'current\\': {\\'last_updated_epoch\\': 1718163900, \\'last_updated\\': \\'2024-06-11 20:45\\', \\'temp_c\\': 20.3, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'temp_f\\': 68.5, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Clear\\', \\'icon\\': </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'//cdn.weatherapi.com/weather/64x64/night/113.png\\', \\'code\\': 1000}, \\'wind_mph\\': 16.1, \\'wind_kph\\': 25.9, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'wind_degree\\': 280, \\'wind_dir\\': \\'W\\', \\'pressure_mb\\': 1010.0, \\'pressure_in\\': 29.82, \\'precip_mm\\': 0.0, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'precip_in\\': 0.0, \\'humidity\\': 59, \\'cloud\\': 0, \\'feelslike_c\\': 20.3, \\'feelslike_f\\': 68.5, \\'windchill_c\\': </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">14.8, \\'windchill_f\\': 58.6, \\'heatindex_c\\': 15.2, \\'heatindex_f\\': 59.4, \\'dewpoint_c\\': 10.8, \\'dewpoint_f\\': </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">51.4, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 5.0, \\'gust_mph\\': 20.6, \\'gust_kph\\': 33.1}}\"}, {\\'url\\': </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'https://world-weather.info/forecast/usa/san_francisco/december-2024/\\', \\'content\\': \\'Extended weather forecast </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in San Francisco. Hourly Week 10 days 14 days 30 days Year. Detailed ⚡ San Francisco Weather Forecast for December</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2024 - day/night 🌡️ temperatures, precipitations - World-Weather.info.\\'}]'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tavily_search_results_json'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_call_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'call_AOSKlG76P36uUTAwuCHGA3mA'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mToolMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'location\\': \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'name\\': \\'San \u001b[0m\n",
       "\u001b[32mFrancisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': \u001b[0m\n",
       "\u001b[32m-122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1718164021, \\'localtime\\': \\'2024-06-11 20:47\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\u001b[0m\n",
       "\u001b[32m\\'current\\': \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'last_updated_epoch\\': 1718163900, \\'last_updated\\': \\'2024-06-11 20:45\\', \\'temp_c\\': 20.3, \u001b[0m\n",
       "\u001b[32m\\'temp_f\\': 68.5, \\'is_day\\': 0, \\'condition\\': \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'text\\': \\'Clear\\', \\'icon\\': \u001b[0m\n",
       "\u001b[32m\\'//cdn.weatherapi.com/weather/64x64/night/113.png\\', \\'code\\': 1000\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, \\'wind_mph\\': 16.1, \\'wind_kph\\': 25.9, \u001b[0m\n",
       "\u001b[32m\\'wind_degree\\': 280, \\'wind_dir\\': \\'W\\', \\'pressure_mb\\': 1010.0, \\'pressure_in\\': 29.82, \\'precip_mm\\': 0.0, \u001b[0m\n",
       "\u001b[32m\\'precip_in\\': 0.0, \\'humidity\\': 59, \\'cloud\\': 0, \\'feelslike_c\\': 20.3, \\'feelslike_f\\': 68.5, \\'windchill_c\\': \u001b[0m\n",
       "\u001b[32m14.8, \\'windchill_f\\': 58.6, \\'heatindex_c\\': 15.2, \\'heatindex_f\\': 59.4, \\'dewpoint_c\\': 10.8, \\'dewpoint_f\\': \u001b[0m\n",
       "\u001b[32m51.4, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 5.0, \\'gust_mph\\': 20.6, \\'gust_kph\\': 33.1\u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'url\\': \u001b[0m\n",
       "\u001b[32m\\'https://world-weather.info/forecast/usa/san_francisco/december-2024/\\', \\'content\\': \\'Extended weather forecast \u001b[0m\n",
       "\u001b[32min San Francisco. Hourly Week 10 days 14 days 30 days Year. Detailed ⚡ San Francisco Weather Forecast for December\u001b[0m\n",
       "\u001b[32m2024 - day/night 🌡️ temperatures, precipitations - World-Weather.info.\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[33mname\u001b[0m=\u001b[32m'tavily_search_results_json'\u001b[0m,\n",
       "        \u001b[33mtool_call_id\u001b[0m=\u001b[32m'call_AOSKlG76P36uUTAwuCHGA3mA'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The current weather in San Francisco is clear with a temperature of 20.3°C (68.5°F). The wind </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">speed is 25.9 km/h coming from the west. The humidity is at 59%, and there is no precipitation.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2531</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2583</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-2ccae1cb-ca80-42b1-a116-1f5c0dd190dc-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2531</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2583</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'The current weather in San Francisco is clear with a temperature of 20.3°C \u001b[0m\u001b[32m(\u001b[0m\u001b[32m68.5°F\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The wind \u001b[0m\n",
       "\u001b[32mspeed is 25.9 km/h coming from the west. The humidity is at 59%, and there is no precipitation.'\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m52\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m2531\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m2583\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "            \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-2ccae1cb-ca80-42b1-a116-1f5c0dd190dc-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m2531\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m52\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m2583\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    # rprint(event)\n",
    "    for v in event.values():\n",
    "        rprint(v[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back a stream of events:\n",
    "\n",
    "> - **first, we get an `AIMessage`, which is the first result from the language model**.\n",
    "\n",
    "```python\n",
    "AIMessage(\n",
    "    content='',\n",
    "    additional_kwargs={\n",
    "        'tool_calls': [{\n",
    "            'id': 'call_eqDDo3U0wuFrWTitkRpMCIw4',\n",
    "            'function': {\n",
    "                'arguments': '{\"query\":\"weather in San Francisco\"}',\n",
    "                'name': 'tavily_search_results_json'}, \n",
    "                'type': 'function'\n",
    "            }]\n",
    "        }, \n",
    "    response_metadata={\n",
    "        'token_usage': {'completion_tokens': 21, 'prompt_tokens': 1186, 'total_tokens': 1207}, \n",
    "        'model_name': 'gpt-3.5-turbo', \n",
    "        'system_fingerprint': None, \n",
    "        'finish_reason': 'tool_calls', \n",
    "        'logprobs': None\n",
    "        }, \n",
    "    id='run-af070dca-58fc-42d1-9d5b-c2d081e72a86-0', \n",
    "    tool_calls=[{\n",
    "        'name': 'tavily_search_results_json', \n",
    "        'args': {'query': 'weather in San Francisco'}, \n",
    "        'id': 'call_eqDDo3U0wuFrWTitkRpMCIw4'\n",
    "        }], \n",
    "    usage_metadata={'input_tokens': 1186, 'output_tokens': 21, 'total_tokens': 1207}\n",
    ")\n",
    "```\n",
    "> - **It tells us to call `tavily`, which is automatically logged aftewards with this printing**:\n",
    "\n",
    "```python\n",
    "Calling: {\n",
    "    'name': 'tavily_search_results_json',\n",
    "    'args': {'query': 'weather in San Francisco'},\n",
    "    'id': 'call_eqDDo3U0wuFrWTitkRpMCIw4'\n",
    "}\n",
    "```\n",
    "> - **Then, the action is performed, is logged with the printing of `Back to the model!`, and we get the following `ToolMessage` (I won't parse it now as it doesn't really improves readability), which is the result of calling `tavily` and, hence, the result of the search**:\n",
    "\n",
    "```python\n",
    "ToolMessage(\n",
    "    content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717751204, \\'localtime\\': \\'2024-06-07 2:06\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717750800, \\'last_updated\\': \\'2024-06-07 02:00\\', \\'temp_c\\': 12.2, \\'temp_f\\': 54.0, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Clear\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/113.png\\', \\'code\\': 1000}, \\'wind_mph\\': 4.3, \\'wind_kph\\': 6.8, \\'wind_degree\\': 10, \\'wind_dir\\': \\'N\\', \\'pressure_mb\\': 1011.0, \\'pressure_in\\': 29.84, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 93, \\'cloud\\': 0, \\'feelslike_c\\': 10.7, \\'feelslike_f\\': 51.2, \\'windchill_c\\': 9.8, \\'windchill_f\\': 49.6, \\'heatindex_c\\': 11.4, \\'heatindex_f\\': 52.6, \\'dewpoint_c\\': 9.3, \\'dewpoint_f\\': 48.8, \\'vis_km\\': 14.0, \\'vis_miles\\': 8.0, \\'uv\\': 1.0, \\'gust_mph\\': 14.0, \\'gust_kph\\': 22.5}}\"}, {\\'url\\': \\'https://www.weather.gov/index.php/mtr/\\', \\'content\\': \\'Current Conditions showing NA; Customize Your Weather.gov. Enter Your City, ST or ZIP Code ... 2024 at 9:40:09 am PDT Watches, Warnings & Advisories. Zoom Out. Excessive Heat Warning. Gale Warning. Heat Advisory. Small Craft Advisory. ... National Weather Service San Francisco Bay Area, CA 21 Grace Hopper Ave, Stop 5 Monterey, CA 93943-5505\\'}]', \n",
    "    name='tavily_search_results_json', \n",
    "    tool_call_id='call_eqDDo3U0wuFrWTitkRpMCIw4'\n",
    ")\n",
    "```\n",
    "> - **Finally, there's an `AIMessage`, which is the result of the LLM, answering our question**:\n",
    "\n",
    "```python\n",
    "AIMessage(\n",
    "    content='The current weather in San Francisco is clear with a temperature of 54.0°F (12.2°C). The wind is blowing at 4.3 mph from the north, and the humidity is at 93%.', \n",
    "    response_metadata={\n",
    "        'token_usage': {'completion_tokens': 46, 'prompt_tokens': 1732, 'total_tokens': 1778}, \n",
    "        'model_name': 'gpt-3.5-turbo', \n",
    "        'system_fingerprint': None, \n",
    "        'finish_reason': 'stop', \n",
    "        'logprobs': None\n",
    "    }, \n",
    "    id='run-de9261f7-2481-4194-b1ac-0e9e5cebaf7b-0', \n",
    "    usage_metadata={'input_tokens': 1732, 'output_tokens': 46, 'total_tokens': 1778}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this `stream` method, we:\n",
    "- get back all of these intermediate results, \n",
    "- have a good visibility of what exactly is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow-Up Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now call this stream with another message.\n",
    "\n",
    "So, this is continuing the same conversation that we had before, with asking a follow-up question.\n",
    "\n",
    "**We don't say anything about the weather, but based on it being a conversation, we would expect it to realize that we're asking about the weather here**.\n",
    "\n",
    "To mention it's the same conversation, of course, we're passing here **the same `thread_id`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'tool_calls'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'call_sswMVjt7V78OMyoQcW1B4Y0z'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'arguments'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'{\"query\":\"weather in Los Angeles\"}'</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tavily_search_results_json'</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>\n",
       "                <span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3161</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3182</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tool_calls'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-6025562f-0aba-47fc-9d30-3a4fbfc1bbda-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tavily_search_results_json'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'weather in Los Angeles'</span><span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'call_sswMVjt7V78OMyoQcW1B4Y0z'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3161</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3182</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'tool_calls'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'id'\u001b[0m: \u001b[32m'call_sswMVjt7V78OMyoQcW1B4Y0z'\u001b[0m,\n",
       "                    \u001b[32m'function'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'arguments'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"query\":\"weather in Los Angeles\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                        \u001b[32m'name'\u001b[0m: \u001b[32m'tavily_search_results_json'\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'type'\u001b[0m: \u001b[32m'function'\u001b[0m\n",
       "                \u001b[1m}\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m21\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m3161\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m3182\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "            \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'tool_calls'\u001b[0m,\n",
       "            \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-6025562f-0aba-47fc-9d30-3a4fbfc1bbda-0'\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'name'\u001b[0m: \u001b[32m'tavily_search_results_json'\u001b[0m,\n",
       "                \u001b[32m'args'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'query'\u001b[0m: \u001b[32m'weather in Los Angeles'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'id'\u001b[0m: \u001b[32m'call_sswMVjt7V78OMyoQcW1B4Y0z'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m3161\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m21\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m3182\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'weather in Los Angeles'}, 'id': 'call_sswMVjt7V78OMyoQcW1B4Y0z'}\n",
      "Back to the model!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToolMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'Los </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Angeles\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 34.05, \\'lon\\': -118.24,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1718164291, \\'localtime\\': \\'2024-06-11 20:51\\'}, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'current\\': {\\'last_updated_epoch\\': 1718163900, \\'last_updated\\': \\'2024-06-11 20:45\\', \\'temp_c\\': 17.2, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'temp_f\\': 63.0, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Overcast\\', \\'icon\\': </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'//cdn.weatherapi.com/weather/64x64/night/122.png\\', \\'code\\': 1009}, \\'wind_mph\\': 3.8, \\'wind_kph\\': 6.1, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'wind_degree\\': 270, \\'wind_dir\\': \\'W\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.91, \\'precip_mm\\': 0.0, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'precip_in\\': 0.0, \\'humidity\\': 78, \\'cloud\\': 100, \\'feelslike_c\\': 17.2, \\'feelslike_f\\': 63.0, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'windchill_c\\': 20.5, \\'windchill_f\\': 68.8, \\'heatindex_c\\': 20.9, \\'heatindex_f\\': 69.7, \\'dewpoint_c\\': 12.9, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'dewpoint_f\\': 55.1, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 6.0, \\'gust_mph\\': 7.2, \\'gust_kph\\': 11.6}}\"},</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{\\'url\\': \\'https://www.accuweather.com/en/us/los-angeles/90012/december-weather/347625\\', \\'content\\': \\'Get the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">monthly weather forecast for Los Angeles, CA, including daily high/low, historical averages, to help you plan </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ahead.\\'}]'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tavily_search_results_json'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_call_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'call_sswMVjt7V78OMyoQcW1B4Y0z'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mToolMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'location\\': \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'name\\': \\'Los \u001b[0m\n",
       "\u001b[32mAngeles\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 34.05, \\'lon\\': -118.24,\u001b[0m\n",
       "\u001b[32m\\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1718164291, \\'localtime\\': \\'2024-06-11 20:51\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32m\\'current\\': \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'last_updated_epoch\\': 1718163900, \\'last_updated\\': \\'2024-06-11 20:45\\', \\'temp_c\\': 17.2, \u001b[0m\n",
       "\u001b[32m\\'temp_f\\': 63.0, \\'is_day\\': 0, \\'condition\\': \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'text\\': \\'Overcast\\', \\'icon\\': \u001b[0m\n",
       "\u001b[32m\\'//cdn.weatherapi.com/weather/64x64/night/122.png\\', \\'code\\': 1009\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, \\'wind_mph\\': 3.8, \\'wind_kph\\': 6.1, \u001b[0m\n",
       "\u001b[32m\\'wind_degree\\': 270, \\'wind_dir\\': \\'W\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.91, \\'precip_mm\\': 0.0, \u001b[0m\n",
       "\u001b[32m\\'precip_in\\': 0.0, \\'humidity\\': 78, \\'cloud\\': 100, \\'feelslike_c\\': 17.2, \\'feelslike_f\\': 63.0, \u001b[0m\n",
       "\u001b[32m\\'windchill_c\\': 20.5, \\'windchill_f\\': 68.8, \\'heatindex_c\\': 20.9, \\'heatindex_f\\': 69.7, \\'dewpoint_c\\': 12.9, \u001b[0m\n",
       "\u001b[32m\\'dewpoint_f\\': 55.1, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 6.0, \\'gust_mph\\': 7.2, \\'gust_kph\\': 11.6\u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32m\\'url\\': \\'https://www.accuweather.com/en/us/los-angeles/90012/december-weather/347625\\', \\'content\\': \\'Get the \u001b[0m\n",
       "\u001b[32mmonthly weather forecast for Los Angeles, CA, including daily high/low, historical averages, to help you plan \u001b[0m\n",
       "\u001b[32mahead.\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[33mname\u001b[0m=\u001b[32m'tavily_search_results_json'\u001b[0m,\n",
       "        \u001b[33mtool_call_id\u001b[0m=\u001b[32m'call_sswMVjt7V78OMyoQcW1B4Y0z'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The current weather in Los Angeles is overcast with a temperature of 17.2°C (63.0°F). The wind </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">speed is 6.1 km/h coming from the west. The humidity is at 78%, and there is no precipitation.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">53</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3663</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3716</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-fabcfb6e-2614-4704-ae77-047f7260e3e7-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3663</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">53</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3716</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'The current weather in Los Angeles is overcast with a temperature of 17.2°C \u001b[0m\u001b[32m(\u001b[0m\u001b[32m63.0°F\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The wind \u001b[0m\n",
       "\u001b[32mspeed is 6.1 km/h coming from the west. The humidity is at 78%, and there is no precipitation.'\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m53\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m3663\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m3716\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "            \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-fabcfb6e-2614-4704-ae77-047f7260e3e7-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m3663\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m53\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m3716\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What about in la?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        rprint(v[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can notice everything works as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Previous Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'San Francisco currently has a temperature of 20.3°C (68.5°F) while Los Angeles has a temperature </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of 17.2°C (63.0°F). Therefore, San Francisco is warmer than Los Angeles at the moment.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3728</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3778</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-3a47caab-cd6c-49a4-a445-881bc5b0777c-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3728</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3778</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'San Francisco currently has a temperature of 20.3°C \u001b[0m\u001b[32m(\u001b[0m\u001b[32m68.5°F\u001b[0m\u001b[32m)\u001b[0m\u001b[32m while Los Angeles has a temperature \u001b[0m\n",
       "\u001b[32mof 17.2°C \u001b[0m\u001b[32m(\u001b[0m\u001b[32m63.0°F\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Therefore, San Francisco is warmer than Los Angeles at the moment.'\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m50\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m3728\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m3778\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "            \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-3a47caab-cd6c-49a4-a445-881bc5b0777c-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m3728\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m50\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m3778\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        rprint(v[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing `thread_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**\n",
    "> \n",
    "> Here...\n",
    "> - at best, we expect the model to **underline there's a problem of context**.\n",
    "> - at worst, we expect it to **hallucinate, doing its best to fulfill the request without a necessary context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'tool_calls'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'call_fT3Ga9y9OQdgiaSVbl7thxc2'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'arguments'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'{\"query\": \"average temperature in Miami\"}'</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tavily_search_results_json'</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'call_qUCiBFtUvOJABP7ID4cvppEz'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'arguments'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'{\"query\": \"average temperature in New York\"}'</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tavily_search_results_json'</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>\n",
       "                <span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">58</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">209</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tool_calls'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-213bb770-24c1-45da-a546-34b51d5f4dad-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tavily_search_results_json'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'average temperature in Miami'</span><span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'call_fT3Ga9y9OQdgiaSVbl7thxc2'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tavily_search_results_json'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'average temperature in New York'</span><span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'call_qUCiBFtUvOJABP7ID4cvppEz'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">58</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">209</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'tool_calls'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'id'\u001b[0m: \u001b[32m'call_fT3Ga9y9OQdgiaSVbl7thxc2'\u001b[0m,\n",
       "                    \u001b[32m'function'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'arguments'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"query\": \"average temperature in Miami\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                        \u001b[32m'name'\u001b[0m: \u001b[32m'tavily_search_results_json'\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'type'\u001b[0m: \u001b[32m'function'\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'id'\u001b[0m: \u001b[32m'call_qUCiBFtUvOJABP7ID4cvppEz'\u001b[0m,\n",
       "                    \u001b[32m'function'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'arguments'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"query\": \"average temperature in New York\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                        \u001b[32m'name'\u001b[0m: \u001b[32m'tavily_search_results_json'\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'type'\u001b[0m: \u001b[32m'function'\u001b[0m\n",
       "                \u001b[1m}\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m58\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m151\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m209\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "            \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'tool_calls'\u001b[0m,\n",
       "            \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-213bb770-24c1-45da-a546-34b51d5f4dad-0'\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'name'\u001b[0m: \u001b[32m'tavily_search_results_json'\u001b[0m,\n",
       "                \u001b[32m'args'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'query'\u001b[0m: \u001b[32m'average temperature in Miami'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'id'\u001b[0m: \u001b[32m'call_fT3Ga9y9OQdgiaSVbl7thxc2'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'name'\u001b[0m: \u001b[32m'tavily_search_results_json'\u001b[0m,\n",
       "                \u001b[32m'args'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'query'\u001b[0m: \u001b[32m'average temperature in New York'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'id'\u001b[0m: \u001b[32m'call_qUCiBFtUvOJABP7ID4cvppEz'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m151\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m58\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m209\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'average temperature in Miami'}, 'id': 'call_fT3Ga9y9OQdgiaSVbl7thxc2'}\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'average temperature in New York'}, 'id': 'call_qUCiBFtUvOJABP7ID4cvppEz'}\n",
      "Back to the model!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToolMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"[{'url': 'https://www.timeanddate.com/weather/usa/miami/climate', 'content': 'Annual Weather </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Averages Near Miami. Averages are for Miami International Airport, which is 8 miles from Miami. Based on weather </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reports collected during 1992-2021. Showing: All Year Climate &amp; Weather Averages in Miami. High Temp: 91 °F. Low </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temp: 61 ...'}, {'url': </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://weatherspark.com/y/18622/Average-Weather-in-Miami-Florida-United-States-Year-Round', 'content': 'The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hottest month of the year in Miami is August, with an average high of 89°F and low of 78°F. The cool season lasts </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for 3.0 months, from December 5 to March 5, with an average daily high temperature below 78°F. The coldest month of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the year in Miami is January, with an average low of 63°F and high of 76°F.'}]\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tavily_search_results_json'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_call_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'call_fT3Ga9y9OQdgiaSVbl7thxc2'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToolMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'[{\\'url\\': \\'https://www.usclimatedata.com/climate/new-york/united-states/3202\\', \\'content\\': </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'Station Data. Monthly averages New York Longitude: -74, Latitude: 40.69 Average weather New York, NY - 11201. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Monthly: 1981-2010 normals\\'}, {\\'url\\': \\'https://www.timeanddate.com/weather/usa/new-york/climate\\', \\'content\\':</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'All Year Climate &amp; Weather Averages in New York\\\\nHigh Temp:\\\\n85\\\\xa0°F\\\\nLow Temp:\\\\n27\\\\xa0°F\\\\nMean </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temp:\\\\n55\\\\xa0°F\\\\nPrecipitation:\\\\n2.03\"\\\\nHumidity:\\\\n62%\\\\nDew Point:\\\\n42\\\\xa0°F\\\\nWind:\\\\n6 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mph\\\\nPressure:\\\\n30.04 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nJanuary Climate &amp; Weather Averages in New York\\\\nHigh </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temp:\\\\n40\\\\xa0°F\\\\nLow Temp:\\\\n27\\\\xa0°F\\\\nMean Temp:\\\\n34\\\\xa0°F\\\\nPrecipitation:\\\\n1.98\"\\\\nHumidity:\\\\n61%\\\\nDew</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Point:\\\\n21\\\\xa0°F\\\\nWind:\\\\n8 mph\\\\nPressure:\\\\n30.08 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nFebruary Climate &amp; Weather </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Averages in New York\\\\nHigh Temp:\\\\n42\\\\xa0°F\\\\nLow Temp:\\\\n28\\\\xa0°F\\\\nMean </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temp:\\\\n35\\\\xa0°F\\\\nPrecipitation:\\\\n1.47\"\\\\nHumidity:\\\\n57%\\\\nDew Point:\\\\n21\\\\xa0°F\\\\nWind:\\\\n8 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mph\\\\nPressure:\\\\n30.06 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nMarch Climate &amp; Weather Averages in New York\\\\nHigh </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temp:\\\\n50\\\\xa0°F\\\\nLow Temp:\\\\n35\\\\xa0°F\\\\nMean Temp:\\\\n43\\\\xa0°F\\\\nPrecipitation:\\\\n2.08\"\\\\nHumidity:\\\\n57%\\\\nDew</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Point:\\\\n27\\\\xa0°F\\\\nWind:\\\\n8 mph\\\\nPressure:\\\\n30.06 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nApril Climate &amp; Weather </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Averages in New York\\\\nHigh Temp:\\\\n62\\\\xa0°F\\\\nLow Temp:\\\\n45\\\\xa0°F\\\\nMean </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temp:\\\\n53\\\\xa0°F\\\\nPrecipitation:\\\\n2.04\"\\\\nHumidity:\\\\n57%\\\\nDew Point:\\\\n37\\\\xa0°F\\\\nWind:\\\\n7 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mph\\\\nPressure:\\\\n30.00 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nMay Climate &amp; Weather Averages in New York\\\\nHigh </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temp:\\\\n72\\\\xa0°F\\\\nLow Temp:\\\\n54\\\\xa0°F\\\\nMean Temp:\\\\n63\\\\xa0°F\\\\nPrecipitation:\\\\n2.27\"\\\\nHumidity:\\\\n63%\\\\nDew</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Point:\\\\n49\\\\xa0°F\\\\nWind:\\\\n6 mph\\\\nPressure:\\\\n30.00 \"Hg\\\\nVisibility:\\\\n9\\\\xa0mi\\\\nJune Climate &amp; Weather </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Averages in New York\\\\nHigh Temp:\\\\n80\\\\xa0°F\\\\nLow Temp:\\\\n64\\\\xa0°F\\\\nMean </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temp:\\\\n72\\\\xa0°F\\\\nPrecipitation:\\\\n2.47\"\\\\nHumidity:\\\\n65%\\\\nDew Point:\\\\n59\\\\xa0°F\\\\nWind:\\\\n5 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mph\\\\nPressure:\\\\n29.96 \"Hg\\\\nVisibility:\\\\n9\\\\xa0mi\\\\nJuly Climate &amp; Weather Averages in New York\\\\nHigh </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temp:\\\\n85\\\\xa0°F\\\\nLow Temp:\\\\n69\\\\xa0°F\\\\nMean Temp:\\\\n77\\\\xa0°F\\\\nPrecipitation:\\\\n2.36\"\\\\nHumidity:\\\\n64%\\\\nDew</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Point:\\\\n64\\\\xa0°F\\\\nWind:\\\\n4 mph\\\\nPressure:\\\\n29.97 \"Hg\\\\nVisibility:\\\\n9\\\\xa0mi\\\\nAugust Climate &amp; Weather </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Averages in New York\\\\nHigh Temp:\\\\n84\\\\xa0°F\\\\nLow Temp:\\\\n68\\\\xa0°F\\\\nMean </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temp:\\\\n76\\\\xa0°F\\\\nPrecipitation:\\\\n2.02\"\\\\nHumidity:\\\\n66%\\\\nDew Point:\\\\n63\\\\xa0°F\\\\nWind:\\\\n5 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mph\\\\nPressure:\\\\n30.01 \"Hg\\\\nVisibility:\\\\n9\\\\xa0mi\\\\nSeptember Climate &amp; Weather Averages in New York\\\\nHigh </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temp:\\\\n76\\\\xa0°F\\\\nLow Temp:\\\\n61\\\\xa0°F\\\\nMean Temp:\\\\n69\\\\xa0°F\\\\nPrecipitation:\\\\n1.85\"\\\\nHumidity:\\\\n67%\\\\nDew</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Point:\\\\n57\\\\xa0°F\\\\nWind:\\\\n5 mph\\\\nPressure:\\\\n30.06 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nOctober Climate &amp; Weather </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Averages in New York\\\\nHigh Temp:\\\\n64\\\\xa0°F\\\\nLow Temp:\\\\n50\\\\xa0°F\\\\nMean </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temp:\\\\n57\\\\xa0°F\\\\nPrecipitation:\\\\n1.77\"\\\\nHumidity:\\\\n65%\\\\nDew Point:\\\\n45\\\\xa0°F\\\\nWind:\\\\n6 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mph\\\\nPressure:\\\\n30.07 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nNovember Climate &amp; Weather Averages in New York\\\\nHigh </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temp:\\\\n54\\\\xa0°F\\\\nLow Temp:\\\\n41\\\\xa0°F\\\\nMean Temp:\\\\n48\\\\xa0°F\\\\nPrecipitation:\\\\n1.81\"\\\\nHumidity:\\\\n62%\\\\nDew</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Point:\\\\n35\\\\xa0°F\\\\nWind:\\\\n7 mph\\\\nPressure:\\\\n30.11 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nDecember Climate &amp; Weather </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Averages in New York\\\\nHigh Temp:\\\\n44\\\\xa0°F\\\\nLow Temp:\\\\n33\\\\xa0°F\\\\nMean </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temp:\\\\n39\\\\xa0°F\\\\nPrecipitation:\\\\n2.23\"\\\\nHumidity:\\\\n62%\\\\nDew Point:\\\\n26\\\\xa0°F\\\\nWind:\\\\n7 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mph\\\\nPressure:\\\\n30.10 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nQuick Climate Info\\\\nWeather by CustomWeather, © </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2023\\\\nNeed some help?\\\\n Climate &amp; Weather Averages in New York, New York, USA\\\\nAnnual Weather Averages Near New </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">York\\\\nAverages are for New York City - Central Park, which is 5 miles from New York.\\\\n © Time and Date AS </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1995–2023\\\\n© Time and Date AS 1995–2023.\\\\n Based on weather reports collected during 1985–2015.\\\\n\\'}]'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tavily_search_results_json'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_call_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'call_qUCiBFtUvOJABP7ID4cvppEz'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mToolMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32m{\u001b[0m\u001b[32m'url': 'https://www.timeanddate.com/weather/usa/miami/climate', 'content': 'Annual Weather \u001b[0m\n",
       "\u001b[32mAverages Near Miami. Averages are for Miami International Airport, which is 8 miles from Miami. Based on weather \u001b[0m\n",
       "\u001b[32mreports collected during 1992-2021. Showing: All Year Climate & Weather Averages in Miami. High Temp: 91 °F. Low \u001b[0m\n",
       "\u001b[32mTemp: 61 ...'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, \u001b[0m\u001b[32m{\u001b[0m\u001b[32m'url': \u001b[0m\n",
       "\u001b[32m'https://weatherspark.com/y/18622/Average-Weather-in-Miami-Florida-United-States-Year-Round', 'content': 'The \u001b[0m\n",
       "\u001b[32mhottest month of the year in Miami is August, with an average high of 89°F and low of 78°F. The cool season lasts \u001b[0m\n",
       "\u001b[32mfor 3.0 months, from December 5 to March 5, with an average daily high temperature below 78°F. The coldest month of\u001b[0m\n",
       "\u001b[32mthe year in Miami is January, with an average low of 63°F and high of 76°F.'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\"\u001b[0m,\n",
       "        \u001b[33mname\u001b[0m=\u001b[32m'tavily_search_results_json'\u001b[0m,\n",
       "        \u001b[33mtool_call_id\u001b[0m=\u001b[32m'call_fT3Ga9y9OQdgiaSVbl7thxc2'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mToolMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'url\\': \\'https://www.usclimatedata.com/climate/new-york/united-states/3202\\', \\'content\\': \u001b[0m\n",
       "\u001b[32m\\'Station Data. Monthly averages New York Longitude: -74, Latitude: 40.69 Average weather New York, NY - 11201. \u001b[0m\n",
       "\u001b[32mMonthly: 1981-2010 normals\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'url\\': \\'https://www.timeanddate.com/weather/usa/new-york/climate\\', \\'content\\':\u001b[0m\n",
       "\u001b[32m\\'All Year Climate & Weather Averages in New York\\\\nHigh Temp:\\\\n85\\\\xa0°F\\\\nLow Temp:\\\\n27\\\\xa0°F\\\\nMean \u001b[0m\n",
       "\u001b[32mTemp:\\\\n55\\\\xa0°F\\\\nPrecipitation:\\\\n2.03\"\\\\nHumidity:\\\\n62%\\\\nDew Point:\\\\n42\\\\xa0°F\\\\nWind:\\\\n6 \u001b[0m\n",
       "\u001b[32mmph\\\\nPressure:\\\\n30.04 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nJanuary Climate & Weather Averages in New York\\\\nHigh \u001b[0m\n",
       "\u001b[32mTemp:\\\\n40\\\\xa0°F\\\\nLow Temp:\\\\n27\\\\xa0°F\\\\nMean Temp:\\\\n34\\\\xa0°F\\\\nPrecipitation:\\\\n1.98\"\\\\nHumidity:\\\\n61%\\\\nDew\u001b[0m\n",
       "\u001b[32mPoint:\\\\n21\\\\xa0°F\\\\nWind:\\\\n8 mph\\\\nPressure:\\\\n30.08 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nFebruary Climate & Weather \u001b[0m\n",
       "\u001b[32mAverages in New York\\\\nHigh Temp:\\\\n42\\\\xa0°F\\\\nLow Temp:\\\\n28\\\\xa0°F\\\\nMean \u001b[0m\n",
       "\u001b[32mTemp:\\\\n35\\\\xa0°F\\\\nPrecipitation:\\\\n1.47\"\\\\nHumidity:\\\\n57%\\\\nDew Point:\\\\n21\\\\xa0°F\\\\nWind:\\\\n8 \u001b[0m\n",
       "\u001b[32mmph\\\\nPressure:\\\\n30.06 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nMarch Climate & Weather Averages in New York\\\\nHigh \u001b[0m\n",
       "\u001b[32mTemp:\\\\n50\\\\xa0°F\\\\nLow Temp:\\\\n35\\\\xa0°F\\\\nMean Temp:\\\\n43\\\\xa0°F\\\\nPrecipitation:\\\\n2.08\"\\\\nHumidity:\\\\n57%\\\\nDew\u001b[0m\n",
       "\u001b[32mPoint:\\\\n27\\\\xa0°F\\\\nWind:\\\\n8 mph\\\\nPressure:\\\\n30.06 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nApril Climate & Weather \u001b[0m\n",
       "\u001b[32mAverages in New York\\\\nHigh Temp:\\\\n62\\\\xa0°F\\\\nLow Temp:\\\\n45\\\\xa0°F\\\\nMean \u001b[0m\n",
       "\u001b[32mTemp:\\\\n53\\\\xa0°F\\\\nPrecipitation:\\\\n2.04\"\\\\nHumidity:\\\\n57%\\\\nDew Point:\\\\n37\\\\xa0°F\\\\nWind:\\\\n7 \u001b[0m\n",
       "\u001b[32mmph\\\\nPressure:\\\\n30.00 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nMay Climate & Weather Averages in New York\\\\nHigh \u001b[0m\n",
       "\u001b[32mTemp:\\\\n72\\\\xa0°F\\\\nLow Temp:\\\\n54\\\\xa0°F\\\\nMean Temp:\\\\n63\\\\xa0°F\\\\nPrecipitation:\\\\n2.27\"\\\\nHumidity:\\\\n63%\\\\nDew\u001b[0m\n",
       "\u001b[32mPoint:\\\\n49\\\\xa0°F\\\\nWind:\\\\n6 mph\\\\nPressure:\\\\n30.00 \"Hg\\\\nVisibility:\\\\n9\\\\xa0mi\\\\nJune Climate & Weather \u001b[0m\n",
       "\u001b[32mAverages in New York\\\\nHigh Temp:\\\\n80\\\\xa0°F\\\\nLow Temp:\\\\n64\\\\xa0°F\\\\nMean \u001b[0m\n",
       "\u001b[32mTemp:\\\\n72\\\\xa0°F\\\\nPrecipitation:\\\\n2.47\"\\\\nHumidity:\\\\n65%\\\\nDew Point:\\\\n59\\\\xa0°F\\\\nWind:\\\\n5 \u001b[0m\n",
       "\u001b[32mmph\\\\nPressure:\\\\n29.96 \"Hg\\\\nVisibility:\\\\n9\\\\xa0mi\\\\nJuly Climate & Weather Averages in New York\\\\nHigh \u001b[0m\n",
       "\u001b[32mTemp:\\\\n85\\\\xa0°F\\\\nLow Temp:\\\\n69\\\\xa0°F\\\\nMean Temp:\\\\n77\\\\xa0°F\\\\nPrecipitation:\\\\n2.36\"\\\\nHumidity:\\\\n64%\\\\nDew\u001b[0m\n",
       "\u001b[32mPoint:\\\\n64\\\\xa0°F\\\\nWind:\\\\n4 mph\\\\nPressure:\\\\n29.97 \"Hg\\\\nVisibility:\\\\n9\\\\xa0mi\\\\nAugust Climate & Weather \u001b[0m\n",
       "\u001b[32mAverages in New York\\\\nHigh Temp:\\\\n84\\\\xa0°F\\\\nLow Temp:\\\\n68\\\\xa0°F\\\\nMean \u001b[0m\n",
       "\u001b[32mTemp:\\\\n76\\\\xa0°F\\\\nPrecipitation:\\\\n2.02\"\\\\nHumidity:\\\\n66%\\\\nDew Point:\\\\n63\\\\xa0°F\\\\nWind:\\\\n5 \u001b[0m\n",
       "\u001b[32mmph\\\\nPressure:\\\\n30.01 \"Hg\\\\nVisibility:\\\\n9\\\\xa0mi\\\\nSeptember Climate & Weather Averages in New York\\\\nHigh \u001b[0m\n",
       "\u001b[32mTemp:\\\\n76\\\\xa0°F\\\\nLow Temp:\\\\n61\\\\xa0°F\\\\nMean Temp:\\\\n69\\\\xa0°F\\\\nPrecipitation:\\\\n1.85\"\\\\nHumidity:\\\\n67%\\\\nDew\u001b[0m\n",
       "\u001b[32mPoint:\\\\n57\\\\xa0°F\\\\nWind:\\\\n5 mph\\\\nPressure:\\\\n30.06 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nOctober Climate & Weather \u001b[0m\n",
       "\u001b[32mAverages in New York\\\\nHigh Temp:\\\\n64\\\\xa0°F\\\\nLow Temp:\\\\n50\\\\xa0°F\\\\nMean \u001b[0m\n",
       "\u001b[32mTemp:\\\\n57\\\\xa0°F\\\\nPrecipitation:\\\\n1.77\"\\\\nHumidity:\\\\n65%\\\\nDew Point:\\\\n45\\\\xa0°F\\\\nWind:\\\\n6 \u001b[0m\n",
       "\u001b[32mmph\\\\nPressure:\\\\n30.07 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nNovember Climate & Weather Averages in New York\\\\nHigh \u001b[0m\n",
       "\u001b[32mTemp:\\\\n54\\\\xa0°F\\\\nLow Temp:\\\\n41\\\\xa0°F\\\\nMean Temp:\\\\n48\\\\xa0°F\\\\nPrecipitation:\\\\n1.81\"\\\\nHumidity:\\\\n62%\\\\nDew\u001b[0m\n",
       "\u001b[32mPoint:\\\\n35\\\\xa0°F\\\\nWind:\\\\n7 mph\\\\nPressure:\\\\n30.11 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nDecember Climate & Weather \u001b[0m\n",
       "\u001b[32mAverages in New York\\\\nHigh Temp:\\\\n44\\\\xa0°F\\\\nLow Temp:\\\\n33\\\\xa0°F\\\\nMean \u001b[0m\n",
       "\u001b[32mTemp:\\\\n39\\\\xa0°F\\\\nPrecipitation:\\\\n2.23\"\\\\nHumidity:\\\\n62%\\\\nDew Point:\\\\n26\\\\xa0°F\\\\nWind:\\\\n7 \u001b[0m\n",
       "\u001b[32mmph\\\\nPressure:\\\\n30.10 \"Hg\\\\nVisibility:\\\\n10\\\\xa0mi\\\\nQuick Climate Info\\\\nWeather by CustomWeather, © \u001b[0m\n",
       "\u001b[32m2023\\\\nNeed some help?\\\\n Climate & Weather Averages in New York, New York, USA\\\\nAnnual Weather Averages Near New \u001b[0m\n",
       "\u001b[32mYork\\\\nAverages are for New York City - Central Park, which is 5 miles from New York.\\\\n © Time and Date AS \u001b[0m\n",
       "\u001b[32m1995–2023\\\\n© Time and Date AS 1995–2023.\\\\n Based on weather reports collected during 1985–2015.\\\\n\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[33mname\u001b[0m=\u001b[32m'tavily_search_results_json'\u001b[0m,\n",
       "        \u001b[33mtool_call_id\u001b[0m=\u001b[32m'call_qUCiBFtUvOJABP7ID4cvppEz'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The average temperature in Miami is around 89°F in August, which is one of the hottest months. In </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">comparison, the average temperature in New York is around 85°F in July, which is one of the warmest months. Based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on this information, Miami tends to be slightly warmer than New York on average.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2219</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2284</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-57266e7f-2b7e-4d40-99aa-88f29e1777d3-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2219</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2284</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'The average temperature in Miami is around 89°F in August, which is one of the hottest months. In \u001b[0m\n",
       "\u001b[32mcomparison, the average temperature in New York is around 85°F in July, which is one of the warmest months. Based \u001b[0m\n",
       "\u001b[32mon this information, Miami tends to be slightly warmer than New York on average.'\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m65\u001b[0m, \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m2219\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m2284\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-3.5-turbo'\u001b[0m,\n",
       "            \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-57266e7f-2b7e-4d40-99aa-88f29e1777d3-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m2219\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m65\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m2284\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        rprint(v[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, we get the worst scenario, with the model doing its best to get two measures to compare, and getting (without us being able to know why...) temperatures in Miami and New York.\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> For comparison purposes, **it would be interesting to see how a more recent model like -4o would perform**..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "except ImportError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous code doesn't work, it can be fixed with installing [**aiosqlite**](https://pypi.org/project/aiosqlite/), as mentioned in [**AsyncSqliteSaver's documentation**](https://langchain-ai.github.io/langgraph/reference/checkpoints/#asyncsqlitesaver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = AsyncSqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna use a new `\"thread_id\"` to start a brand new conversation.\n",
    "\n",
    "We're also going to be iterating over a different type of event. These events represent updates from the underlying stream.\n",
    "\n",
    "We want to **look for events that correspond to new tokens**.\n",
    "\n",
    "These kind of events are called `\"on_chat_model_stream\"`.\n",
    "\n",
    "When we see these events happening, we want to get the content, and print it out with a pipe delimiter.\n",
    "\n",
    "When running this, we should see it streaming real time into the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "The SqliteSaver does not support async methods. Consider using AsyncSqliteSaver instead.\nfrom langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\nNote: AsyncSqliteSaver requires the aiosqlite package to use.\nInstall with:\n`pip install aiosqlite`\nSee https://langchain-ai.github.io/langgraph/reference/checkpoints/#asyncsqlitesaverfor more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the weather in SF?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      2\u001b[0m thread \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m abot\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mastream_events({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages}, thread, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m     kind \u001b[38;5;241m=\u001b[39m event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_chat_model_stream\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/LangChainDoc/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1137\u001b[0m, in \u001b[0;36mRunnable.astream_events\u001b[0;34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnly versions \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of the schema is currently supported.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1135\u001b[0m     )\n\u001b[0;32m-> 1137\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m event_stream:\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "File \u001b[0;32m~/Documents/LangChainDoc/.venv/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py:638\u001b[0m, in \u001b[0;36m_astream_events_implementation_v1\u001b[0;34m(runnable, input, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m root_name \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, runnable\u001b[38;5;241m.\u001b[39mget_name())\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# Ignoring mypy complaint about too many different union combinations\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# This arises because many of the argument types are unions\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m _astream_log_implementation(  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     runnable,\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    641\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    642\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    643\u001b[0m     diff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    644\u001b[0m     with_streamed_output_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    646\u001b[0m ):\n\u001b[1;32m    647\u001b[0m     run_log \u001b[38;5;241m=\u001b[39m run_log \u001b[38;5;241m+\u001b[39m log\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m encountered_start_event:\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;66;03m# Yield the start event for the root runnable.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/LangChainDoc/.venv/lib/python3.11/site-packages/langchain_core/tracers/log_stream.py:637\u001b[0m, in \u001b[0;36m_astream_log_implementation\u001b[0;34m(runnable, input, config, stream, diff, with_streamed_output_list, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# Wait for the runnable to finish, if not cancelled (eg. by break)\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m task\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/LangChainDoc/.venv/lib/python3.11/site-packages/langchain_core/tracers/log_stream.py:591\u001b[0m, in \u001b[0;36m_astream_log_implementation.<locals>.consume_astream\u001b[0;34m()\u001b[0m\n\u001b[1;32m    588\u001b[0m prev_final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    589\u001b[0m final_output: Optional[Output] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m runnable\u001b[38;5;241m.\u001b[39mastream(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    592\u001b[0m     prev_final_output \u001b[38;5;241m=\u001b[39m final_output\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/LangChainDoc/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1098\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1095\u001b[0m processes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes}\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;66;03m# get checkpoint from saver, or create an empty one\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m saved \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer\u001b[38;5;241m.\u001b[39maget_tuple(config)\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m )\n\u001b[1;32m   1102\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m saved\u001b[38;5;241m.\u001b[39mcheckpoint \u001b[38;5;28;01mif\u001b[39;00m saved \u001b[38;5;28;01melse\u001b[39;00m empty_checkpoint()\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;66;03m# merge configurable fields with previous checkpoint config\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/LangChainDoc/.venv/lib/python3.11/site-packages/langgraph/checkpoint/sqlite.py:401\u001b[0m, in \u001b[0;36mSqliteSaver.aget_tuple\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maget_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[CheckpointTuple]:\n\u001b[1;32m    395\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a checkpoint tuple from the database asynchronously.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    Note:\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m        This async method is not supported by the SqliteSaver class.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m        Use get_tuple() instead, or consider using [AsyncSqliteSaver](#asyncsqlitesaver).\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(_AIO_ERROR_MSG)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The SqliteSaver does not support async methods. Consider using AsyncSqliteSaver instead.\nfrom langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\nNote: AsyncSqliteSaver requires the aiosqlite package to use.\nInstall with:\n`pip install aiosqlite`\nSee https://langchain-ai.github.io/langgraph/reference/checkpoints/#asyncsqlitesaverfor more information."
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            # Empty content in the context of OpenAI means\n",
    "            # that the model is asking for a tool to be invoked.\n",
    "            # So we only print non-empty content\n",
    "            print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
